"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿå®‰è£…æ–‡æ¡£

1. ç¯å¢ƒè¦æ±‚ï¼š
   - Python 3.7+
   - pip (PythonåŒ…ç®¡ç†å™¨)

2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ä½†æ¨èï¼‰ï¼š
   python -m venv venv
   source venv/bin/activate  # åœ¨Windowsä¸Šä½¿ç”¨: venv\Scripts\activate

3. å®‰è£…ä¾èµ–ï¼š
   pip install -r requirements.txt

4. requirements.txt æ–‡ä»¶å†…å®¹ï¼š
   streamlit
   openai
   sentence-transformers
   PyPDF2
   python-docx
   faiss-cpu
   tiktoken
   serpapi
   pandas
   sqlite3  # é€šå¸¸å·²åŒ…å«åœ¨Pythonæ ‡å‡†åº“ä¸­

5. å…¶ä»–ä¾èµ–ï¼š
   - ç¡®ä¿ä½ æœ‰æœ‰æ•ˆçš„OpenAI APIå¯†é’¥
   - å¦‚æœä½¿ç”¨Googleæœç´¢åŠŸèƒ½ï¼Œéœ€è¦æœ‰æ•ˆçš„SerpAPIå¯†é’¥

6. è¿è¡Œåº”ç”¨ï¼š
   streamlit run app.py

æ³¨æ„ï¼š
- è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å®‰è£…
- åœ¨ä»£ç ä¸­æ›¿æ¢OpenAI APIå¯†é’¥å’ŒSerpAPIå¯†é’¥ä¸ºä½ è‡ªå·±çš„å¯†é’¥
- å¯¹äºå¤§å‹æ–‡ä»¶å¤„ç†ï¼Œå¯èƒ½éœ€è¦å¢åŠ ç³»ç»Ÿå†…å­˜æˆ–ä½¿ç”¨æ›´å¼ºå¤§çš„ç¡¬ä»¶
"""

import streamlit as st
import sys
import os
import logging
import json

# è®¾ç½®é¡µé¢é…ç½®å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ - by Huaiyuan Tan",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# æ·»åŠ å¼€å‘è€…ä¿¡æ¯
st.markdown("<h6 style='text-align: right; color: gray;'>å¼€å‘è€…: Huaiyuan Tan</h6>", unsafe_allow_html=True)

# éšè— Streamlit é»˜è®¤çš„èœå•ã€é¡µè„šå’Œ Deploy æŒ‰é’®
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

from openai import OpenAI
from sentence_transformers import SentenceTransformer
import multiprocessing
import PyPDF2
import docx
import faiss
import tiktoken
import os
import pickle
import numpy as np
import jieba
from collections import Counter
import pandas as pd
from neo4j_operations import get_neo4j_connection, AURA_CONFIG, LOCAL_CONFIG
from data_processor import export_to_neo4j, query_imported_data, DataProcessor
from graph_builder import GraphBuilder
from graph_query import GraphQuery

# åˆå§‹åŒ–
client = OpenAI(
    api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
    base_url="https://api.chatanywhere.tech/v1"
)

@st.cache_resource
def load_model():
    with st.spinner('æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...'):
        return SentenceTransformer('paraphrase-MiniLM-L3-v2')  # ä½¿ç”¨æ›´å°çš„æ¨¡å‹

# å…¨å±€å˜é‡æ¥å­˜å‚¨æ¨¡å‹
model = None

# è®¡ç®—tokenæ•°é‡
def num_tokens_from_string(string: str) -> int:
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(string))

# æ–‡æ¡£å‘é‡åŒ–æ¨¡
def vectorize_document(file, max_tokens, model):
    text = ""
    if file.type == "application/pdf":
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = docx.Document(file)
        for para in doc.paragraphs:
            text += para.text + "\n"
    else:
        text = file.getvalue().decode("utf-8")
    
    chunks = []
    current_chunk = ""
    for sentence in text.split('.'):
        if num_tokens_from_string(current_chunk + sentence) > max_tokens:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
        else:
            current_chunk += sentence + '.'
    if current_chunk:
        chunks.append(current_chunk)
    
    vectors = model.encode(chunks)
    index = faiss.IndexFlatL2(384)  # 384æ˜¯å‘é‡ç»´åº¦,æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
    index.add(vectors)
    return chunks, index

# æ–°å¢å‡½æ•°ï¼šæå–å…³é”®è¯
def extract_keywords(text, top_k=5):
    words = jieba.cut(text)
    word_count = Counter(words)
    # è¿‡æ»¤æ‰åœç”¨è¯å’Œä¸ªå­—ç¬¦
    keywords = [word for word, count in word_count.most_common(top_k*2) if len(word) > 1]
    return keywords[:top_k]

# æ–°å¢å‡½æ•°ï¼šåŸºäºå…³é”®è¯æœç´¢æ–‡æ¡£
def search_documents(keywords, file_indices):
    relevant_docs = []
    for file_name, (chunks, _) in file_indices.items():
        doc_content = ' '.join(chunks)
        if any(keyword in doc_content for keyword in keywords):
            relevant_docs.append(file_name)
    return relevant_docs

# ä¿®æ”¹çŸ¥è¯†é—®ç­”æ¨¡å—
def rag_qa(query, file_indices, relevant_docs=None):
    keywords = extract_keywords(query)
    if relevant_docs is None:
        relevant_docs = search_documents(keywords, file_indices)
    
    if not relevant_docs:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚", [], ""

    all_chunks = []
    chunk_to_file = {}
    combined_index = faiss.IndexFlatL2(384)
    
    offset = 0
    for file_name in relevant_docs:
        if file_name in file_indices:
            chunks, index = file_indices[file_name]
            all_chunks.extend(chunks)
            for i in range(len(chunks)):
                chunk_to_file[offset + i] = file_name
            combined_index.add(index.reconstruct_n(0, index.ntotal))
            offset += len(chunks)

    if not all_chunks:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚", [], ""

    query_vector = model.encode([query])
    D, I = combined_index.search(query_vector, k=3)
    context = []
    context_with_sources = []
    for i in I[0]:
        if 0 <= i < len(all_chunks):  # ç¡®ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            chunk = all_chunks[i]
            context.append(chunk)
            file_name = chunk_to_file.get(i, "æœªçŸ¥æ–‡ä»¶")
            context_with_sources.append((file_name, chunk))

    context_text = "\n".join(context)
    
    # ç¡®ä¿æ€»tokenæ•°ä¸è¶…è¿‡4096
    max_context_tokens = 3000  # ä¸ºç³»ç»Ÿæ¶ˆæ¯ã€æŸ¥è¯¢å’Œå…¶ä»–å†…å®¹é¢„ç•™æ›´å¤šç©ºé—´
    while num_tokens_from_string(context_text) > max_context_tokens:
        context_text = context_text[:int(len(context_text)*0.9)]  # æ¯æ¬¡å‡å°‘10%çš„å†…å®¹
    
    if not context_text:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", [], ""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œæ— è®ºé—®é¢˜æ˜¯ä»€ä¹ˆè¯­è¨€ã€‚åœ¨å›ç­”ä¹‹åï¼Œè¯·åŠ¡å¿…æä¾›ä¸€ä¸ªæœ€ç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"},
            {"role": "user", "content": f"ä¸Šä¸‹æ–‡: {context_text}\n\né—®é¢˜: {query}\n\nè¯·æä¾›ä½ çš„å›ç­”ç„¶ååœ¨å›ç­”åé¢é™„ä¸Šç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"}
        ]
    )
    answer = response.choices[0].message.content
    
    # æ›´çµæ´»åœ°å¤„ç†å›ç­”æ ¼å¼
    if "ç›¸å…³åŸæ–‡ï¼š" in answer:
        answer_parts = answer.split("ç›¸å…³åŸæ–‡ï¼š", 1)
        main_answer = answer_parts[0].strip()
        relevant_excerpt = answer_parts[1].strip()
    else:
        main_answer = answer.strip()
        relevant_excerpt = ""
    
    # å¦‚æœAIæ²¡æœ‰æä¾›ç›¸å…³åŸæ–‡ï¼Œæˆ‘ä»¬ä»ä¸Šä¸‹æ–‡ä¸­é€‰æ‹©ä¸€ä¸ª
    if not relevant_excerpt and context:
        relevant_excerpt = context[0][:200] + "..."  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡çš„å‰200ä¸ªå­—ç¬¦
    
    # æ‰¾å‡ºåŒ…å«ç›¸å…³åŸæ–‡çš„æ–‡ä»¶
    relevant_sources = []
    if relevant_excerpt:
        for file_name, chunk in context_with_sources:
            if relevant_excerpt in chunk:
                relevant_sources.append((file_name, chunk))
                break  # åªæ·»åŠ ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
    if not relevant_sources and context_with_sources:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡æº
        relevant_sources.append(context_with_sources[0])

    return main_answer, relevant_sources, relevant_excerpt

# ä¿å­˜ç´¢å¼•å’Œchunks
def save_index(file_name, chunks, index):
    if not os.path.exists('indices'):
        os.makedirs('indices')
    with open(f'indices/{file_name}.pkl', 'wb') as f:
        pickle.dump((chunks, index), f)
    # ä¿å­˜æ–‡ä»¶ååˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
    else:
        file_list = []
    if file_name not in file_list:
        file_list.append(file_name)
        with open(file_list_path, 'w') as f:
            f.write('\n'.join(file_list))

# åŠ è½½æ‰€æœ‰ä¿å­˜çš„ç´¢å¼•
def load_all_indices():
    file_indices = {}
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
        for file_name in file_list:
            file_path = f'indices/{file_name}.pkl'
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    chunks, index = pickle.load(f)
                file_indices[file_name] = (chunks, index)
    return file_indices

def delete_index(file_name):
    if os.path.exists(f'indices/{file_name}.pkl'):
        os.remove(f'indices/{file_name}.pkl')
    file_list_path = 'indices/file_list.txt'
    if os.path.exists(file_list_path):
        with open(file_list_path, 'r') as f:
            file_list = f.read().splitlines()
        if file_name in file_list:
            file_list.remove(file_name)
            with open(file_list_path, 'w') as f:
                f.write('\n'.join(file_list))

def combined_qa(query, file_indices, neo4j_conn, relevant_docs=None):
    # ä½¿ç”¨FAISSæ£€ç´¢
    faiss_response, faiss_sources, faiss_excerpt = rag_qa(query, file_indices, relevant_docs)
    
    # ä½¿ç”¨Neo4jæ£€ç´¢
    graph_query = GraphQuery(neo4j_conn)
    neo4j_response, neo4j_sources, neo4j_excerpt = graph_query.neo4j_qa(query)
    
    # åˆå¹¶ç»“æœ
    combined_context = f"FAISSç»“æœï¼š{faiss_response}\n\nNeo4jç»“æœï¼š{neo4j_response}"
    
    # ä½¿ç”¨åˆå¹¶åçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç»™å®šçš„FAISSå’ŒNeo4jçš„ç»“æœï¼Œç»¼åˆåˆ†æå¹¶ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œæ— è®ºé—®é¢˜æ˜¯ä»€ä¹ˆè¯­è¨€ã€‚åœ¨å›ç­”ä¹‹åï¼Œè¯·åŠ¡å¿…æä¾›ä¸€ä¸ªæœ€ç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"},
            {"role": "user", "content": f"ä¸Šä¸‹æ–‡: {combined_context}\n\né—®é¢˜: {query}\n\nè¯·æä¾›ä½ çš„ç»¼åˆå›ç­”ï¼Œç„¶ååœ¨å›ç­”åé¢é™„ä¸Šæœ€ç›¸å…³çš„åŸæ–‡æ‘˜å½•ï¼Œä»¥'ç›¸å…³åŸæ–‡ï¼š'ä¸ºå‰ç¼€ã€‚"}
        ]
    )
    final_answer = response.choices[0].message.content
    
    # å¤„ç†å›ç­”æ ¼å¼
    if "ç›¸å…³åŸæ–‡ï¼š" in final_answer:
        answer_parts = final_answer.split("ç›¸å…³åŸæ–‡ï¼š", 1)
        main_answer = answer_parts[0].strip()
        relevant_excerpt = answer_parts[1].strip()
    else:
        main_answer = final_answer.strip()
        relevant_excerpt = faiss_excerpt or neo4j_excerpt
    
    # åˆå¹¶æº
    combined_sources = list(set(faiss_sources + neo4j_sources))
    
    return main_answer, combined_sources, relevant_excerpt

def main():
    global model  # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
    
    st.markdown("""
    <style>
    .reportview-container .main .block-container{
        padding-top: 1rem;
        padding-right: 1rem;
        padding-left: 1rem;
        padding-bottom: 1rem;
    }
    .stColumn {
        padding: 5px;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")

    # åŠ Neo4jè¿æ¥é€‰æ‹©
    st.sidebar.title("Neo4jè¿æ¥è®¾ç½®")
    connection_type = st.sidebar.radio("æ‹©Neo4jè¿æ¥ç±»å‹", ("æœ¬åœ°", "Aura"))
    
    if connection_type == "æœ¬åœ°":
        neo4j_config = LOCAL_CONFIG
    else:
        neo4j_config = AURA_CONFIG
    
    # åˆ›å»ºNeo4jè¿æ¥
    try:
        neo4j_conn = get_neo4j_connection(neo4j_config)
        
        # æ‰§è¡Œç®€å•æŸ¥è¯¢æ¥éªŒè¯è¿æ¥
        result = neo4j_conn.query("CALL dbms.components() YIELD name, versions, edition UNWIND versions as version RETURN name, version, edition;")
        if result:
            db_info = result[0]
            st.sidebar.success(f"æˆåŠŸè¿æ¥åˆ° {connection_type} Neo4jæ•°æ®åº“")
            st.sidebar.info(f"æ•°æ®åº“åç§°: {db_info['name']}")
            st.sidebar.info(f"ç‰ˆæœ¬: {db_info['version']}")
            st.sidebar.info(f"ç‰ˆæœ¬: {db_info['edition']}")
        else:
            st.sidebar.warning(f"å·²è¿æ¥åˆ° {connection_type} Neo4jæ•°æ®åº“ï¼Œä½†æ— æ³•è·å–æ•°æ®åº“ä¿¡æ¯")

        # æ·»åŠ æµ‹è¯•æŒ‰é’®
        if st.sidebar.button("æµ‹è¯•Neo4jè¿æ¥"):
            try:
                # åˆ›å»ºæµ‹è¯•èŠ‚ç‚¹
                create_result = neo4j_conn.create_node("TestNode", {"name": "Test", "timestamp": str(pd.Timestamp.now())})
                st.sidebar.success("æˆåŠŸåˆ›å»ºæµ‹è¯•èŠ‚ç‚¹")

                # æŸ¥è¯¢æ‰€æœ‰TestNode
                query_result = neo4j_conn.query("MATCH (n:TestNode) RETURN n")
                st.sidebar.write("æŸ¥è¯¢ç»“æœ:")
                for record in query_result:
                    st.sidebar.write(record['n'])

            except Exception as e:
                st.sidebar.error(f"æµ‹è¯•Neo4jè¿æ¥æ—¶å‡ºé”™: {str(e)}")

    except Exception as e:
        st.sidebar.error(f"è¿æ¥ {connection_type} Neo4jæ•°æ®åº“å¤±è´¥: {str(e)}")
        neo4j_conn = None

    # åˆå§‹åŒ– session state
    if "rag_messages" not in st.session_state:
        st.session_state.rag_messages = []
    if "file_indices" not in st.session_state:
        st.session_state.file_indices = load_all_indices()
    
    # æå‰åŠ è½½æ¨¡å‹
    model = load_model()

    st.header("RAG é—®ç­”")

    # æ–‡æ¡£ä¸Šä¼ éƒ¨åˆ†
    st.subheader("æ–‡æ¡£ä¸Šä¼ ")
    
    max_tokens = 4096

    uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True, key="rag_file_uploader_1")

    if uploaded_files:
        for uploaded_file in uploaded_files:
            with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {uploaded_file.name}..."):
                chunks, index = vectorize_document(uploaded_file, max_tokens, model)  # ä¼ å…¥ model
                st.session_state.file_indices[uploaded_file.name] = (chunks, index)
                save_index(uploaded_file.name, chunks, index)
            st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²å‘é‡åŒ–å¹¶æ·»åŠ åˆ°ç´¢å¼•ä¸­ï¼")

    # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡ä»¶å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
    st.subheader("å·²å¤„ç†æ–‡æ¡£:")
    for file_name in list(st.session_state.file_indices.keys()):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.write(f"â€¢ {file_name}")
        with col2:
            if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                del st.session_state.file_indices[file_name]
                delete_index(file_name)
                st.success(f"æ–‡æ¡£ {file_name} å·²åˆ é™¤ï¼")
                st.rerun()

    # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
    st.subheader("å…³é”®è¯æœç´¢")
    search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰", key="rag_search_keywords_1")
    if search_keywords:
        keywords = search_keywords.split()
        relevant_docs = search_documents(keywords, st.session_state.file_indices)
        if relevant_docs:
            st.write("ç›¸å…³æ–‡æ¡£ï¼š")
            for doc in relevant_docs:
                st.write(f"â€¢ {doc}")
            st.session_state.relevant_docs = relevant_docs
        else:
            st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
            st.session_state.relevant_docs = None

    # æ·»åŠ æ£€ç´¢æ–¹æ³•é€‰æ‹©
    st.sidebar.subheader("æ£€ç´¢æ–¹æ³•é€‰æ‹©")
    retrieval_method = st.sidebar.radio(
        "é€‰æ‹©æ£€ç´¢æ–¹æ³•",
        ("FAISS", "Neo4j", "FAISS + Neo4j")
    )

    # å¯¹è¯éƒ¨åˆ†
    st.subheader("å¯¹è¯")
    chat_container = st.container()

    with chat_container:
        for i, message in enumerate(st.session_state.rag_messages):
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "sources" in message and message["sources"]:
                    st.markdown("**å‚è€ƒæ¥æºï¼š**")
                    file_name, _ = message["sources"][0]
                    st.markdown(f"**æ–‡ä»¶ï¼š** {file_name}")
                    if os.path.exists(f'indices/{file_name}.pkl'):
                        with open(f'indices/{file_name}.pkl', 'rb') as f:
                            file_content = pickle.load(f)[0]  # è·å–æ–‡ä»¶å†…å®¹
                        st.download_button(
                            label="ä¸‹è½½æºæ–‡",
                            data='\n'.join(file_content),
                            file_name=file_name,
                            mime='text/plain',
                            key=f"download_{i}"
                        )
                if "relevant_excerpt" in message:
                    st.markdown(f"**ç›¸å…³åŸæ–‡ï¼š** <mark>{message['relevant_excerpt']}</mark>", unsafe_allow_html=True)

    # ç”¨æˆ·è¾“å…¥
    prompt = st.chat_input("è¯·åŸºäºä¸Šä¼ çš„æ–‡æ¡£æå‡ºé¢˜:", key="rag_chat_input_1")

    if prompt:
        st.session_state.rag_messages.append({"role": "user", "content": prompt})
        
        if st.session_state.file_indices:
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)
                with st.chat_message("assistant"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                        try:
                            relevant_docs = st.session_state.get('relevant_docs')
                            
                            # æ ¹æ®é€‰æ‹©çš„æ£€ç´¢æ–¹æ³•æ‰§è¡Œä¸åŒçš„æ£€ç´¢
                            if retrieval_method == "FAISS":
                                response, sources, relevant_excerpt = rag_qa(prompt, st.session_state.file_indices, relevant_docs)
                            elif retrieval_method == "Neo4j":
                                graph_query = GraphQuery(neo4j_conn)
                                response, sources, relevant_excerpt = graph_query.neo4j_qa(prompt)
                            else:  # FAISS + Neo4j
                                response, sources, relevant_excerpt = combined_qa(prompt, st.session_state.file_indices, neo4j_conn, relevant_docs)
                            
                            st.markdown(response)
                            if sources:
                                st.markdown("**å‚è€ƒæ¥æºï¼š**")
                                file_name, _ = sources[0]
                                st.markdown(f"**æ–‡ä»¶ï¼š** {file_name}")
                                if os.path.exists(f'indices/{file_name}.pkl'):
                                    with open(f'indices/{file_name}.pkl', 'rb') as f:
                                        file_content = pickle.load(f)[0]  # è·å–æ–‡ä»¶å†…å®¹
                                    st.download_button(
                                        label="ä¸‹è½½æºæ–‡ä»¶",
                                        data='\n'.join(file_content),
                                        file_name=file_name,
                                        mime='text/plain',
                                        key=f"download_new_{len(st.session_state.rag_messages)}"
                                    )
                            if relevant_excerpt:
                                st.markdown(f"**ç›¸å…³åŸæ–‡ï¼š** <mark>{relevant_excerpt}</mark>", unsafe_allow_html=True)
                            else:
                                st.warning("æœªèƒ½æå–åˆ°ç²¾ç¡®çš„ç›¸å…³åŸæ–‡ï¼Œä½†æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
                        except Exception as e:
                            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.session_state.rag_messages.append({
                    "role": "assistant", 
                    "content": response, 
                    "sources": sources,
                    "relevant_excerpt": relevant_excerpt
                })
        else:
            with chat_container:
                with st.chat_message("assistant"):
                    st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚")

    if st.sidebar.button("å¯¼å‡ºæ•°æ®åˆ°Neo4jå¹¶æ„å»ºå›¾è°±"):
        if neo4j_conn and st.session_state.file_indices:
            try:
                # è®¾ç½®æ—¥å¿—çº§åˆ«
                logging.basicConfig(level=logging.INFO)
                logger = logging.getLogger(__name__)

                # ä½¿ç”¨ DataProcessor å¤„ç†æ•°æ®
                processor = DataProcessor()
                processed_data = processor.process_file_indices(st.session_state.file_indices)
                
                # æ·»åŠ æ—¥å¿—æ¥æ£€æŸ¥å¤„ç†åçš„æ•°æ®
                logger.info(f"Processed data: {processed_data}")

                # ä½¿ç”¨ GraphBuilder æ„å»ºå›¾è°±
                graph_builder = GraphBuilder(neo4j_conn)
                node_count, rel_count, next_relationships_count = graph_builder.build_graph(processed_data)

                st.sidebar.success(f"æˆåŠŸå¯¼å‡ºæ•°æ®åˆ°Neo4jå¹¶æ„å»ºå›¾è°±ã€‚")
                st.sidebar.info(f"åˆ›å»ºçš„TextChunkèŠ‚ç‚¹æ•°é‡: {node_count}")
                st.sidebar.info(f"åˆ›å»ºçš„NEXTå…³ç³»æ•°é‡: {rel_count}")
                st.sidebar.info(f"å°è¯•åˆ›å»ºçš„NEXTå…³ç³»æ•°é‡: {next_relationships_count}")

                # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹èŠ‚ç‚¹å’Œå…³ç³»
                st.sidebar.subheader("ç¤ºä¾‹èŠ‚ç‚¹:")
                sample_nodes = neo4j_conn.query("MATCH (n:TextChunk) RETURN n LIMIT 5")
                for node in sample_nodes:
                    try:
                        node_data = dict(node['n'])
                        # å¦‚æœ embedding æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„ JSONï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è§£æå®ƒ
                        if 'embedding' in node_data and isinstance(node_data['embedding'], str):
                            node_data['embedding'] = json.loads(node_data['embedding'])
                        st.sidebar.json(node_data)
                    except Exception as e:
                        st.sidebar.error(f"Error displaying node: {str(e)}")

                st.sidebar.subheader("ç¤ºä¾‹å…³ç³»:")
                sample_relationships = neo4j_conn.query("MATCH (a:TextChunk)-[r:NEXT]->(b:TextChunk) RETURN a.chunk_id, b.chunk_id, type(r) LIMIT 5")
                for rel in sample_relationships:
                    st.sidebar.write(f"{rel['a.chunk_id']} -[{rel['type(r)']}]-> {rel['b.chunk_id']}")

            except Exception as e:
                st.sidebar.error(f"å¯¼å‡ºæ•°æ®åˆ°Neo4jå¹¶æ„å»ºå›¾è°±æ—¶å‡ºé”™: {str(e)}")
                logger.error(f"Error during graph building: {str(e)}", exc_info=True)
        else:
            st.sidebar.warning("è¯·ç¡®ä¿å·²è¿æ¥åˆ°Neo4jå¹¶ä¸Šä¼ äº†æ–‡æ¡£ã€‚")

    # åœ¨mainå‡½æ•°ç»“æŸå‰å…³é—­Neo4jè¿æ¥
    if neo4j_conn:
        neo4j_conn.close()

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
if __name__ == "__main__":
    main()
else:
    # å¯¼å‡ºå…³é”®å‡½æ•°
    __all__ = ['load_model', 'vectorize_document', 'save_index', 'load_all_indices', 'delete_index', 'rag_qa']