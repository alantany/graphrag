"""
AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ
"""

import streamlit as st
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import multiprocessing
import PyPDF2
import docx
import faiss
import tiktoken
import os
import pickle
import numpy as np
import jieba
from collections import Counter
import io  # æ·»åŠ è¿™ä¸€è¡Œ
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
from data_processor import (
    load_model, vectorize_document, extract_keywords, 
    search_documents, save_index, load_all_indices, 
    delete_index, rag_qa, initialize_openai,
    query_graph, hybrid_search, get_entity_relations,
    set_neo4j_config, get_neo4j_driver, process_data,
    generate_final_answer, vector_search, execute_neo4j_query,
    initialize_faiss  # æ·»åŠ è¿™ä¸€è¡Œ
)

# åœ¨æ–‡ä»¶é¡¶éƒ¨çš„å¯¼å…¥è¯­å¥ä¹‹åæ·»åŠ 
from data_processor import faiss_id_to_text, faiss_id_counter, faiss_index

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# éšè— Streamlit é»˜è®¤çš„èœå•ã€é¡µè„šå’Œ Deploy æŒ‰é’®
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
initialize_openai(
    api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
    base_url="https://api.chatanywhere.tech/v1"
)

# åˆå§‹åŒ– session state
if "file_indices" not in st.session_state:
    st.session_state.file_indices = load_all_indices()

def main():
    global faiss_id_to_text, faiss_id_counter, faiss_index
    
    st.title("AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")

    # åˆå§‹åŒ– FAISS
    try:
        faiss_index = initialize_faiss()
        if faiss_index is None:
            st.error("FAISS ç´¢å¼•åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æ£€æŸ¥ initialize_faiss() å‡½æ•°ã€‚")
            return
    except Exception as e:
        st.error(f"FAISS ç´¢å¼•åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return

    # åŠ è½½æ‰€æœ‰ç´¢å¼•
    st.session_state.file_indices = load_all_indices()
    
    # å¦‚æœæœ‰ç´¢å¼•ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ° FAISS ç´¢å¼•ä¸­
    if st.session_state.file_indices:
        for file_name, (chunks, index) in st.session_state.file_indices.items():
            for i, chunk in enumerate(chunks):
                faiss_id_to_text[faiss_id_counter + i] = chunk
            vectors = index.reconstruct_n(0, index.ntotal)
            faiss_index.add(vectors)
        faiss_id_counter += sum(len(chunks) for chunks, _ in st.session_state.file_indices.values())

    # Neo4j é…ç½®é€‰æ‹©
    neo4j_option = st.radio(
        "é€‰æ‹© Neo4j è¿æ¥æ–¹å¼",
        ("Neo4j Aura", "æœ¬åœ° Neo4j")
    )

    if neo4j_option == "Neo4j Aura":
        CURRENT_NEO4J_CONFIG = set_neo4j_config("AURA")
        st.success("å·²é€‰æ‹©å¹¶è¿æ¥åˆ° Neo4j Aura")
    else:
        CURRENT_NEO4J_CONFIG = set_neo4j_config("LOCAL")
        st.success("å·²é€‰æ‹©å¹¶è¿æ¥åˆ°æœ¬åœ° Neo4j")

    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if CURRENT_NEO4J_CONFIG:
        try:
            driver = get_neo4j_driver()
            with driver.session() as session:
                result = session.run("RETURN 1 AS test")
                test_value = result.single()["test"]
                if test_value == 1:
                    st.success("Neo4j æ•°æ®åº“è¿æ¥æˆåŠŸ")
                else:
                    st.error("Neo4j æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            driver.close()
        except Exception as e:
            st.error(f"è¿æ¥åˆ° Neo4j æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
    else:
        st.error("Neo4j é…ç½®æ— æ•ˆæˆ–æœªè®¾ç½®")

    # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿
    st.markdown("---")

    # åˆ›å»ºä¸‰ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs(["æ–‡æ¡£ä¸Šä¼ ", "çŸ¥è¯†åº“é—®ç­”", "æ•°æ®åº“æ£€ç´¢"])

    with tab1:
        st.header("æ–‡æ¡£ä¸Šä¼ ")
        
        # è®¾ç½®æœ€å¤§tokenæ•°
        max_tokens = 4096

        # å¤šæ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True)

        if uploaded_files:
            for uploaded_file in uploaded_files:
                content = uploaded_file.read()
                if uploaded_file.type == "application/pdf":
                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
                    content = ""
                    for page in pdf_reader.pages:
                        content += page.extract_text()
                elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    doc = docx.Document(io.BytesIO(content))
                    content = "\n".join([para.text for para in doc.paragraphs])
                else:
                    content = content.decode('utf-8')
                
                st.write(f"æ–‡ä»¶ '{uploaded_file.name}' å·²ä¸Šä¼ ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("åŠ è½½åˆ°å›¾æ•°æ®åº“", key=f"graph_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å›¾æ•°æ®åº“: {uploaded_file.name}..."):
                            result = process_data(content)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å›¾æ•°æ®åº“ï¼")
                        st.write(f"å¤„ç†äº† {len(result['entities'])} ä¸ªå®ä½“å’Œ {len(result['relations'])} ä¸ªå…³ç³»")
                        
                        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
                        with st.expander("æŸ¥çœ‹è¯¦ç»†å¤„ç†ç»“æœ"):
                            st.subheader("å®ä½“:")
                            for entity in result['entities']:
                                st.write(f"- {entity}")
                            st.subheader("å…³ç³»:")
                            for relation in result['relations']:
                                st.write(f"- {relation['source']} --[{relation['relation']}]--> {relation['target']}")
                
                with col2:
                    if st.button("åŠ è½½åˆ°å‘é‡æ•°æ®åº“", key=f"vector_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å‘é‡æ•°æ®åº“: {uploaded_file.name}..."):
                            chunks, index = vectorize_document(content, max_tokens)
                            st.session_state.file_indices[uploaded_file.name] = (chunks, index)
                            save_index(uploaded_file.name, chunks, index)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å‘é‡æ•°æ®åº“ï¼")
                        st.write(f"å‘ FAISS å‘é‡æ•°æ®åº“æ·»åŠ äº† {len(chunks)} ä¸ªæ–‡æœ¬æ®µè½")

        # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡æ¡£å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
        st.subheader("å·²å¤„ç†æ–‡æ¡£:")
        for file_name in list(st.session_state.file_indices.keys()):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"â€¢ {file_name}")
            with col2:
                if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                    del st.session_state.file_indices[file_name]
                    delete_index(file_name)
                    st.success(f"æ–‡æ¡£ {file_name} å·²åˆ é™¤ï¼")
                    st.rerun()

    with tab2:
        st.header("çŸ¥è¯†åº“é—®ç­”")
        
        # é€‰æ‹©ç­”ç±»å‹
        qa_type = st.radio("é€‰æ‹©é—®ç­”ç±»å‹", ["å‘é‡æ•°æ®åº“é—®ç­”", "å›¾æ•°æ®åº“é—®ç­”", "æ··åˆé—®ç­”"])
        
        if qa_type == "å‘é‡æ•°æ®åº“é—®ç­”":
            # å‘é‡æ•°æ®åº“é—®ç­”éƒ¨åˆ†
            st.subheader("å‘é‡æ•°æ®åº“é—®ç­”")
            vector_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå‘é‡æ•°æ®åº“ï¼‰")
            if st.button("æäº¤é—®é¢˜ï¼ˆå‘é‡ï¼‰"):
                if vector_query:
                    with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                        answer, sources, excerpt = rag_qa(vector_query, st.session_state.file_indices)
                    st.write("å›ç­”ï¼š", answer)
                    if sources:
                        st.write("å‚è€ƒæ¥æºï¼š")
                        for source, _ in sources:
                            st.write(f"- {source}")
                    if excerpt:
                        st.write("ç›¸å…³åŸæ–‡ï¼š")
                        st.write(excerpt)
                else:
                    st.warning("è¯·å…¥é—®é¢˜")
        
        elif qa_type == "å›¾æ•°æ®åº“é—®ç­”":
            # å›¾æ•°æ®åº“é—®ç­”éƒ¨åˆ†
            st.subheader("å›¾æ•°æ®åº“é—®ç­”")
            graph_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå›¾æ•°æ®åº“ï¼‰")
            if st.button("æäº¤é—®é¢˜ï¼ˆå›¾ï¼‰"):
                if graph_query:
                    with st.spinner("æ­£åœ¨ï¿½ï¿½è¯¢..."):
                        answer = hybrid_search(graph_query)
                    st.write("å›ç­”ï¼š", answer)
                else:
                    st.warning("è¯·è¾“å…¥é—®é¢˜")
        
        else:
            # æ··åˆé—®ç­”éƒ¨åˆ†
            st.subheader("æ··åˆé—®ç­”")
            hybrid_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆæ··åˆé—®ç­”ï¼‰")
            if st.button("æäº¤é—®é¢˜ï¼ˆæ··åˆï¼‰"):
                if hybrid_query:
                    with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                        # å›¾æ•°æ®åº“æŸ¥è¯¢
                        graph_answer = hybrid_search(hybrid_query)
                        
                        # å‘é‡æ•°æ®åº“æŸ¥è¯¢
                        vector_answer, sources, excerpt = rag_qa(hybrid_query, st.session_state.file_indices)
                        
                        # ç»„åˆç»“æœ
                        combined_context = f"å›¾æ•°æ®åº“å›ç­”ï¼š{graph_answer}\n\nå‘é‡æ•°æ®åº“å›ç­”ï¼š{vector_answer}"
                        if excerpt:
                            combined_context += f"\n\nç›¸å…³åŸæ–‡ï¼š{excerpt}"
                        
                        # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                        final_answer = generate_final_answer(hybrid_query, combined_context)
                        
                        st.write("æœ€ç»ˆå›ç­”ï¼š", final_answer)
                        st.write("å›¾æ•°æ®åº“å›ç­”ï¼š", graph_answer)
                        st.write("å‘é‡æ•°æ®åº“å›ç­”ï¼š", vector_answer)
                        if sources:
                            st.write("å‚è€ƒæ¥æºï¼š")
                            for source, _ in sources:
                                st.write(f"- {source}")
                        if excerpt:
                            st.write("ç›¸å…³åŸæ–‡ï¼š")
                            st.write(excerpt)
                else:
                    st.warning("è¯·è¾“å…¥é—®é¢˜")

        # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
        st.subheader("å…³é”®è¯æœç´¢")
        search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰")
        if search_keywords:
            keywords = search_keywords.split()
            relevant_docs = search_documents(keywords, st.session_state.file_indices)
            if relevant_docs:
                st.write("ç›¸å…³æ–‡æ¡£ï¼š")
                for doc in relevant_docs:
                    st.write(f"â€¢ {doc}")
                # å­˜å‚¨ç›¸å…³æ–‡æ¡£åˆ° session state
                st.session_state.relevant_docs = relevant_docs
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                st.session_state.relevant_docs = None

    with tab3:
        st.header("æ•°æ®åº“æ£€ç´¢")
        
        search_type = st.radio("é€‰æ‹©æœç´¢ç±»å‹", ["å›¾æ•°æ®åº“æœç´¢", "å‘é‡æ•°æ®åº“æœç´¢", "Neo4j å‘½ä»¤æ‰§è¡Œ"])
        
        if search_type == "å›¾æ•°æ®åº“æœç´¢":
            st.subheader("å›¾æ•°æ®åº“æœç´¢")
            graph_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
            if st.button("æ‰§è¡Œå›¾æ•°æ®åº“æœç´¢"):
                if graph_query:
                    with st.spinner("æ­£åœ¨æœç´¢å›¾æ•°æ®åº“..."):
                        entities, relations, contents = query_graph(graph_query)
                    if entities or relations:
                        st.success("æœç´¢å®Œæˆï¼")
                        st.write("æ‰¾åˆ°çš„å®ä½“:")
                        st.write(", ".join(entities))
                        st.write("ç›¸å…³å…³ç³»:")
                        for relation in relations:
                            st.write(f"{relation['source']} --[{relation['relation']}]--> {relation['target']}")
                        
                        # åˆ›å»ºå¹¶æ˜¾ç¤ºå…³ç³»å›¾
                        G = nx.Graph()
                        for entity in entities:
                            G.add_node(entity)
                        for relation in relations:
                            G.add_edge(relation['source'], relation['target'], title=relation['relation'])
                        
                        net = Network(notebook=True, width="100%", height="500px", bgcolor="#222222", font_color="white")
                        net.from_nx(G)
                        net.save_graph("graph.html")
                        
                        with open("graph.html", 'r', encoding='utf-8') as f:
                            html_string = f.read()
                        components.html(html_string, height=600)
                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
                else:
                    st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ã€‚")

        elif search_type == "å‘é‡æ•°æ®åº“æœç´¢":
            st.subheader("å‘é‡æ•°æ®åº“æœç´¢")
            vector_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
            if st.button("æ‰§è¡Œå‘é‡æ•°æ®åº“æœç´¢"):
                if vector_query:
                    with st.spinner("æ­£åœ¨æœç´¢å‘é‡æ•°æ®åº“..."):
                        results = vector_search(vector_query, k=5)  # å‡è®¾ k=5ï¼Œè¿”å‰5ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
                    if results:
                        st.success("æœç´¢å®Œæˆï¼")
                        for i, result in enumerate(results, 1):
                            st.write(f"ç»“æœ {i}:")
                            st.write(f"ç›¸ä¼¼åº¦: {1 - result['distance']:.4f}")
                            st.write(f"å†…å®¹: {result['text'][:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
                            st.write("---")
                    else:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
                else:
                    st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ã€‚")

        else:  # Neo4j å‘½ä»¤æ‰§è¡Œ
            st.subheader("Neo4j å‘½ä»¤æ‰§è¡Œ")
            cypher_query = st.text_area("è¾“å…¥ Cypher æŸ¥è¯¢è¯­å¥")
            if st.button("æ‰§è¡Œ Neo4j æŸ¥è¯¢"):
                if cypher_query:
                    with st.spinner("æ­£åœ¨æ‰§è¡Œ Neo4j æŸ¥è¯¢..."):
                        try:
                            results = execute_neo4j_query(cypher_query)
                            if results:
                                st.success("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼")
                                df = pd.DataFrame(results)
                                st.dataframe(df)
                            else:
                                st.info("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰è¿”å›ç»“æœã€‚")
                        except Exception as e:
                            st.error(f"æ‰§è¡ŒæŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                else:
                    st.warning("è¯·è¾“å…¥ Cypher æŸ¥è¯¢è¯­å¥ã€‚")

if __name__ == "__main__":
    main()