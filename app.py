"""
åŸºäºå›¾æ•°æ®åº“åŠå‘é‡æ•°æ®åº“çš„æ··åˆé—®ç­”ç³»ç»Ÿ
"""

import streamlit as st
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import multiprocessing
import PyPDF2
import docx
import faiss
import tiktoken
import os
import pickle
import numpy as np
import jieba
from collections import Counter
import io
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
import re  # æ·»åŠ è¿™ä¸€è¡Œ
from data_processor import (
    load_model, vectorize_document, extract_keywords, 
    search_documents, save_index, load_all_indices, 
    delete_index, rag_qa, initialize_openai,
    query_graph, hybrid_search, get_entity_relations,
    set_neo4j_config, get_neo4j_driver, process_data,
    generate_final_answer, vector_search, execute_neo4j_query,
    initialize_faiss, create_fulltext_index, search_fulltext_index,
    open_dir, delete_graph_data, delete_vector_data, delete_fulltext_index
)

# åœ¨æ–‡ä»¶é¡¶éƒ¨çš„å¯¼å…¥è¯­å¥ä¹‹åæ·»åŠ 
from data_processor import faiss_id_to_text, faiss_id_counter, faiss_index

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# éšè— Streamlit é»˜è®¤çš„èœå•ã€é¡µè„šå’Œ Deploy æŒ‰é’®
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
initialize_openai(
    api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
    base_url="https://api.chatanywhere.tech/v1"
)

# åˆå§‹åŒ– session state
if "file_indices" not in st.session_state:
    st.session_state.file_indices = load_all_indices()

def decompose_query(query):
    prompt = f"""
    è¯·å°†ä»¥ä¸‹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªç®€å•å­æŸ¥è¯¢ï¼š
    {query}
    
    è¾“å‡ºæ ¼å¼ï¼š
    1. å­æŸ¥è¯¢1
    2. å­æŸ¥è¯¢2
    ...
    """
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸“é—¨äºåˆ†è§£å¤æ‚æŸ¥è¯¢çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip().split("\n")

def main():
    if 'faiss_id_to_text' not in st.session_state:
        st.session_state.faiss_id_to_text = {}
    if 'faiss_id_counter' not in st.session_state:
        st.session_state.faiss_id_counter = 0
    
    st.title("AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")

    # åˆå§‹åŒ– FAISS
    try:
        faiss_index = initialize_faiss()
        if faiss_index is None:
            st.error("FAISS ç´¢å¼•åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æ£€æŸ¥ initialize_faiss() å‡½æ•°ã€‚")
            return
    except Exception as e:
        st.error(f"FAISS ç´¢å¼•åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return

    # åŠ è½½æ‰€æœ‰ç´¢å¼•
    st.session_state.file_indices = load_all_indices()
    
    # å¦‚æœæœ‰ç´¢å¼•ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ° FAISS ç´¢å¼•ä¸­
    if st.session_state.file_indices:
        for file_name, (chunks, index) in st.session_state.file_indices.items():
            for i, chunk in enumerate(chunks):
                st.session_state.faiss_id_to_text[st.session_state.faiss_id_counter + i] = chunk
            vectors = index.reconstruct_n(0, index.ntotal)
            faiss_index.add(vectors)
        st.session_state.faiss_id_counter += sum(len(chunks) for chunks, _ in st.session_state.file_indices.values())

    # Neo4j é…ç½®é€‰æ‹©
    col1, col2, col3 = st.columns(3)
    with col1:
        neo4j_option = st.radio(
            "é€‰æ‹© Neo4j è¿æ¥æ–¹å¼",
            ("Neo4j Aura", "æœ¬åœ° Neo4j")
        )

    with col2:
        if neo4j_option == "Neo4j Aura":
            CURRENT_NEO4J_CONFIG = set_neo4j_config("AURA")
            connection_status = "å·²é€‰æ‹©è¿æ¥åˆ° Neo4j Aura"
        else:
            CURRENT_NEO4J_CONFIG = set_neo4j_config("LOCAL")
            connection_status = "å·²é€‰æ‹©è¿æ¥åˆ°æœ¬åœ° Neo4j"

    with col3:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        if CURRENT_NEO4J_CONFIG:
            try:
                driver = get_neo4j_driver()
                with driver.session() as session:
                    result = session.run("RETURN 1 AS test")
                    test_value = result.single()["test"]
                    if test_value == 1:
                        connection_status += " - è¿æ¥æˆåŠŸ"
                    else:
                        connection_status += " - è¿æ¥æµ‹è¯•å¤±è´¥"
                driver.close()
            except Exception as e:
                connection_status += f" - è¿æ¥é”™è¯¯: {str(e)}"
        else:
            connection_status += " - é…ç½®æ— æ•ˆæˆ–æœªè®¾ç½®"

    st.write(connection_status)

    # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿
    st.markdown("---")

    # åˆ›å»ºå››ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["æ–‡æ¡£ä¸Šä¼ ", "çŸ¥è¯†åº“é—®ç­”", "æ•°æ®åº“æ£€ç´¢", "æ•°æ®ç®¡ç†"])

    with tab1:
        st.header("æ–‡æ¡£ä¸Šä¼ ")
        
        # è®¾ç½®æœ€å¤§tokenæ•°
        max_tokens = 4096

        # å¤šæ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True)

        if uploaded_files:
            for uploaded_file in uploaded_files:
                content = uploaded_file.read()
                if uploaded_file.type == "application/pdf":
                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
                    content = ""
                    for page in pdf_reader.pages:
                        content += page.extract_text()
                elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    doc = docx.Document(io.BytesIO(content))
                    content = "\n".join([para.text for para in doc.paragraphs])
                else:
                    content = content.decode('utf-8')
                
                st.write(f"æ–‡ä»¶ '{uploaded_file.name}' å·²ä¸Šä¼ ")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("åŠ è½½åˆ°å›¾æ•°æ®åº“", key=f"graph_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å›¾æ•°æ®åº“: {uploaded_file.name}..."):
                            result = process_data(content)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å›¾æ•°æ®åº“ï¼")
                        st.write(f"å¤„ç†äº† {len(result['entities'])} ä¸ªå®ä½“å’Œ {len(result['relations'])} ä¸ªå…³ç³»")
                        
                        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
                        with st.expander("æŸ¥çœ‹è¯¦ç»†å¤„ç†ç»“æœ"):
                            st.subheader("å®ä½“:")
                            for entity in result['entities']:
                                st.write(f"- {entity}")
                            st.subheader("å…³ç³»:")
                            for relation in result['relations']:
                                st.write(f"- {relation['source']} --[{relation['relation']}]--> {relation['target']}")
                
                with col2:
                    if st.button("åŠ è½½åˆ°å‘é‡æ•°æ®åº“", key=f"vector_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å‘é‡æ•°æ®åº“: {uploaded_file.name}..."):
                            chunks, index = vectorize_document(content, max_tokens)
                            st.session_state.file_indices[uploaded_file.name] = (chunks, index)
                            save_index(uploaded_file.name, chunks, index)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å‘é‡æ•°æ®åº“ï¼")
                        st.write(f"å‘ FAISS å‘é‡æ•°æ®åº“æ·»åŠ äº† {len(chunks)} ä¸ªæ–‡æœ¬æ®µè½")
                
                with col3:
                    if st.button("åˆ›å»ºå…¨æ–‡ç´¢å¼•", key=f"fulltext_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨ä¸ºæ–‡æ¡£åˆ›å»ºå…¨æ–‡ç´¢å¼•: {uploaded_file.name}..."):
                            try:
                                create_fulltext_index(content, uploaded_file.name)
                                st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåˆ›å»ºå…¨æ–‡ç´¢å¼•ï¼")
                                # æ·»åŠ éªŒè¯æ­¥éª¤
                                ix = open_dir("fulltext_index")
                                with ix.searcher() as searcher:
                                    results = searcher.search(QueryParser("title", ix.schema).parse(uploaded_file.name))
                                    if results:
                                        st.success(f"æˆåŠŸéªŒè¯æ–‡æ¡£ {uploaded_file.name} å·²è¢«ç´¢å¼•")
                                        st.info(f"ç´¢å¼•ä¸­çš„æ–‡æ¡£å†…å®¹é•¿åº¦: {sum(len(hit['content']) for hit in results)}")
                                        st.info(f"ç´¢å¼•ä¸­çš„ç¬¬ä¸€ä¸ªæ–‡æ¡£å—å†…å®¹å‰200å­—ç¬¦: {results[0]['content'][:200]}")
                                    else:
                                        st.warning(f"æ— æ³•åœ¨ç´¢å¼•ä¸­æ‰¾åˆ°æ–‡æ¡£ {uploaded_file.name}")
                            except Exception as e:
                                st.error(f"åˆ›å»ºæˆ–éªŒè¯ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
                                st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                                st.error(f"é”™è¯¯è¯¦æƒ…: {e.args}")

        # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡æ¡£å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
        st.subheader("å·²å¤„ç†æ–‡æ¡£:")
        for file_name in list(st.session_state.file_indices.keys()):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"â€¢ {file_name}")
            with col2:
                if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                    with st.spinner(f"æ­£åœ¨åˆ é™¤æ–‡æ¡£ {file_name} çš„æ‰€æœ‰ç›¸å…³æ•°æ®..."):
                        try:
                            # åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®
                            delete_vector_data(file_name)
                            
                            # åˆ é™¤å›¾æ•°æ®åº“ä¸­çš„æ•°æ®
                            delete_graph_data(file_name)
                            
                            # åˆ é™¤å…¨æ–‡ç´¢å¼•ä¸­çš„æ•°æ®
                            delete_fulltext_index(file_name)
                            
                            # åˆ é™¤æœ¬åœ°ç´¢å¼•æ–‡ä»¶
                            delete_index(file_name)
                            
                            # éªŒè¯åˆ é™¤æ“ä½œ
                            ix = open_dir("fulltext_index")
                            with ix.searcher() as searcher:
                                remaining_docs = [doc for doc in searcher.all_stored_fields() if doc['title'].startswith(file_name)]
                                if not remaining_docs:
                                    st.success(f"æ–‡æ¡£ {file_name} åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®å·²æˆåŠŸåˆ é™¤ï¼")
                                else:
                                    st.warning(f"æ–‡æ¡£ {file_name} çš„ä¸€äº›ç›¸å…³æ•°æ®å¯èƒ½æ²¡æœ‰å®Œå…¨åˆ é™¤ã€‚æ­£åœ¨å°è¯•æ¸…ç†...")
                                    for doc in remaining_docs:
                                        delete_fulltext_index(doc['title'])
                                    st.success(f"æ¸…ç†å®Œæˆã€‚è¯·æ£€æŸ¥ç´¢å¼•ä»¥ç¡®ä¿æ‰€æœ‰ç›¸å…³æ•°æ®å·²è¢«åˆ é™¤ã€‚")
                        except Exception as e:
                            st.error(f"åˆ é™¤æ–‡æ¡£ {file_name} æ—¶å‡ºé”™: {str(e)}")
                            logger.error(f"åˆ é™¤æ–‡æ¡£ {file_name} æ—¶å‡ºé”™", exc_info=True)
                    st.rerun()

    with tab2:
        st.header("çŸ¥è¯†åº“é—®ç­”")
        
        qa_type = st.radio("é€‰æ‹©é—®ç­”ç±»å‹", ["å‘é‡æ•°æ®åº“é—®ç­”", "å›¾æ•°æ®åº“é—®ç­”", "æ··åˆé—®ç­”"])
        
        if qa_type == "å‘é‡æ•°æ®åº“é—®ç­”":
            st.subheader("å‘é‡æ•°æ®åº“é—®ç­”")
            with st.form(key='vector_qa_form'):
                vector_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå‘é‡æ•°æ®åº“ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and vector_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    answer, sources, excerpt = rag_qa(vector_query, st.session_state.file_indices)
                st.write("å›ç­”ï¼š", answer)
                if sources:
                    st.write("å‚è€ƒæ¥æºï¼š")
                    for source, _ in sources:
                        st.write(f"- {source}")
                if excerpt:
                    st.write("ç›¸å…³åŸæ–‡ï¼š")
                    st.write(excerpt)
        
        elif qa_type == "å›¾æ•°æ®åº“é—®ç­”":
            st.subheader("å›¾æ•°æ®åº“é—®ç­”")
            with st.form(key='graph_qa_form'):
                graph_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é¢˜ï¼ˆå›¾æ•°æ®åº“ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and graph_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    answer = hybrid_search(graph_query)
                st.write("å›ç­”ï¼š", answer)
        
        else:  # æ··åˆé—®ç­”
            st.subheader("æ··åˆé—®ç­”")
            with st.form(key='hybrid_qa_form'):
                hybrid_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆæ··åˆé—®ç­”ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and hybrid_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    # å…¨æ–‡æ£€ç´¢
                    try:
                        fulltext_results = search_fulltext_index(hybrid_query)
                        st.write(f"å…¨æ–‡ç´¢å¼•ä¸­å…±æœ‰ {len(fulltext_results)} ä¸ªç›¸å…³æ–‡æ¡£è¢«æœç´¢")
                    except Exception as e:
                        st.error(f"å…¨æ–‡æ£€ç´¢å‡ºé”™: {str(e)}")
                        fulltext_results = []

                    # å›¾æ•°æ®åº“æŸ¥è¯¢
                    graph_answer, graph_entities, graph_relations = hybrid_search(hybrid_query)
                    
                    # å‘é‡æ•°æ®åº“æŸ¥
                    vector_answer, sources, excerpt = rag_qa(hybrid_query, st.session_state.file_indices)
                    
                    # ä½¿ç”¨æ‰€æœ‰ç»“æœæˆæœ€ç»ˆç­”æ¡ˆ
                    final_answer = generate_final_answer(
                        hybrid_query, 
                        graph_answer, 
                        vector_answer, 
                        fulltext_results,  # ç¡®ä¿è¿™é‡Œä¼ é€’äº† fulltext_results
                        excerpt, 
                        graph_entities, 
                        graph_relations
                    )
                    
                    st.write("æœ€ç»ˆå›ç­”ï¼š", final_answer)
                    st.write("å›¾æ•°æ®åº“å›ç­”ï¼š", graph_answer)
                    st.write("å‘é‡æ•°æ®åº“å›ç­”ï¼š", vector_answer)
                    st.write(f"å…¨æ–‡æ£€ç´¢ç»“æœæ•°é‡ï¼š{len(fulltext_results)}")

                    # æ˜¾ç¤ºå…¨æ–‡æ£€ç»“æœ
                    if fulltext_results:
                        st.write("å…¨æ–‡æ£€ç´¢ç»“æœï¼ˆå‰3ä¸ªï¼‰ï¼š")
                        for result in fulltext_results[:3]:
                            st.write(f"- æ–‡æ¡£: {result['title']}, ç›¸å…³åº¦: {result['score']:.2f}")
                            highlights = result['highlights']
                            # å¤„ç†é«˜äº®æ–‡æœ¬
                            highlights = re.sub(r'<b class="match term\d+">', '**', highlights)
                            highlights = highlights.replace('</b>', '**')
                            # å°†è¿ç»­çš„æ˜Ÿå·åˆå¹¶
                            highlights = re.sub(r'\*{2,}', '**', highlights)
                            # ç§»é™¤å¯èƒ½æ®‹ç•™çš„HTMLæ ‡ç­¾
                            highlights = re.sub(r'<[^>]+>', '', highlights)
                            st.markdown(f"  åŒ¹é…å†…å®¹: {highlights}")
                            st.write(f"  æ–‡æ¡£å†…å®¹ç‰‡æ®µ: {result['content']}")
                    else:
                        st.write("å…¨æ–‡æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")

        # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
        st.subheader("å…³é”®è¯æœç´¢")
        with st.form(key='keyword_search_form'):
            search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰")
            submit_button = st.form_submit_button(label='æœç´¢')
        if submit_button and search_keywords:
            keywords = search_keywords.split()
            relevant_docs = search_documents(keywords, st.session_state.file_indices)
            if relevant_docs:
                st.write("ç›¸å…³æ–‡æ¡£ï¼š")
                for doc in relevant_docs:
                    st.write(f" {doc}")
                # å­˜å‚¨ç›¸å…³æ–‡æ¡£åˆ° session state
                st.session_state.relevant_docs = relevant_docs
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                st.session_state.relevant_docs = None

    with tab3:
        st.header("æ•°æ®åº“æ£€ç´¢")
        
        search_type = st.radio("é€‰æ‹©æœç´¢ç±»å‹", ["å›¾æ•°æ®åº“æœç´¢", "å‘é‡æ•°æ®åº“æœç´¢", "å…¨æ–‡ç´¢å¼•æœç´¢", "Neo4j å‘½ä»¤æ‰§è¡Œ"])
        
        if search_type == "å›¾æ•°æ®åº“æœç´¢":
            st.subheader("å›¾æ•°åº“æœç´¢")
            with st.form(key='graph_search_form'):
                graph_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå›¾æ•°æ®åº“æœç´¢')
            if submit_button and graph_query:
                with st.spinner("æ­£åœ¨æœç´¢å›¾æ•°æ®åº“..."):
                    entities, relations, contents = query_graph(graph_query)
                if entities or relations:
                    st.success("æœç´¢å®Œæˆ")
                    st.write("æ‰¾åˆ°çš„å®ä½“:")
                    st.write(", ".join(entities))
                    st.write("ç›¸å…³å…³ç³»:")
                    for relation in relations:
                        st.write(f"{relation['source']} --[{relation['relation']}]--> {relation['target']}")
                    
                    # åˆ›å»ºå¹¶æ˜¾ç¤ºå…³ç³»å›¾
                    G = nx.Graph()
                    for entity in entities:
                        G.add_node(entity)
                    for relation in relations:
                        G.add_edge(relation['source'], relation['target'], title=relation['relation'])
                    
                    net = Network(notebook=True, width="100%", height="500px", bgcolor="#222222", font_color="white")
                    net.from_nx(G)
                    net.save_graph("graph.html")
                    
                    with open("graph.html", 'r', encoding='utf-8') as f:
                        html_string = f.read()
                    components.html(html_string, height=600)
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

        elif search_type == "å‘é‡æ•°æ®åº“æœç´¢":
            st.subheader("å‘é‡æ•°æ®åº“æœç´¢")
            with st.form(key='vector_search_form'):
                vector_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå‘é‡æ•°æ®åº“æœç´¢')
            if submit_button and vector_query:
                with st.spinner("æ­£åœ¨æœç´¢å‘é‡æ•°æ®åº“..."):
                    results = vector_search(vector_query, k=5)  # å‡è®¾ k=5ï¼Œè¿”å‰5ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
                if results:
                    st.success("æœç´¢å®Œæˆï¼")
                    for i, result in enumerate(results, 1):
                        st.write(f"ç»“æœ {i}:")
                        st.write(f"ç›¸ä¼¼åº¦: {1 - result['distance']:.4f}")
                        st.write(f"å†…å®¹: {result['text'][:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
                        st.write("---")
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

        elif search_type == "å…¨æ–‡ç´¢å¼•æœç´¢":
            st.subheader("å…¨æ–‡ç´¢å¼•æœç´¢")
            with st.form(key='fulltext_search_form'):
                fulltext_query = st.text_input("è¾“æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå…¨æ–‡ç´¢å¼•æœç´¢')
            if submit_button and fulltext_query:
                with st.spinner("æ­£åœ¨æœç´¢å…¨æ–‡ç´¢å¼•..."):
                    try:
                        results = search_fulltext_index(fulltext_query)
                        if results:
                            st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
                            for result in results:
                                st.write(f"æ–‡æ¡£: {result['title']}, ç›¸å…³åº¦: {result['score']:.2f}")
                                highlights = result['highlights']
                                # å¤„ç†é«˜äº®æ–‡æœ¬
                                highlights = re.sub(r'<b class="match term\d+">', '**', highlights)
                                highlights = highlights.replace('</b>', '**')
                                # å°†è¿ç»­çš„æ˜Ÿå·åˆå¹¶
                                highlights = re.sub(r'\*{2,}', '**', highlights)
                                # ç§»é™¤å¯èƒ½æ®‹ç•™çš„HTMLæ ‡ç­¾
                                highlights = re.sub(r'<[^>]+>', '', highlights)
                                st.markdown(f"åŒ¹é…å†…å®¹: {highlights}")
                                st.write(f"æ–‡æ¡£å†…å®¹ç‰‡æ®µ: {result['content']}")
                                st.write("---")
                        else:
                            st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    except Exception as e:
                        st.error(f"æœç´¢å…¨æ–‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")

        else:  # Neo4j å‘½ä»¤æ‰§è¡Œ
            st.subheader("Neo4j å‘½ä»¤æ‰§è¡Œ")
            with st.form(key='neo4j_query_form'):
                cypher_query = st.text_area("è¾“å…¥ Cypher æŸ¥è¯¢è¯­å¥")
                submit_button = st.form_submit_button(label='æ‰§è¡Œ Neo4j æŸ¥è¯¢')
            if submit_button and cypher_query:
                with st.spinner("æ­£åœ¨æ‰§è¡Œ Neo4j æŸ¥è¯¢..."):
                    try:
                        results = execute_neo4j_query(cypher_query)
                        if results:
                            st.success("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼")
                            df = pd.DataFrame(results)
                            st.dataframe(df)
                        else:
                            st.info("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰è¿”å›ç»“æœã€‚")
                    except Exception as e:
                        st.error(f"æ‰§è¡ŒæŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    with tab4:
        st.header("æ•°æ®ç®¡ç†")

        col1, col2 = st.columns(2)

        with col1:
            # å…¨æ–‡ç´¢å¼•ä¿¡æ¯
            if st.button("å…¨æ–‡ç´¢å¼•ä¿¡æ¯"):
                with st.spinner("æ­£åœ¨è·å–å…¨æ–‡ç´¢å¼•ä¿¡æ¯..."):
                    try:
                        ix = open_dir("fulltext_index")
                        with ix.searcher() as searcher:
                            all_docs = list(searcher.all_stored_fields())
                            st.write(f"å…¨æ–‡ç´¢å¼•ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£")
                            for doc in all_docs:
                                st.write(f"- æ–‡æ¡£: {doc['title']}")
                    except Exception as e:
                        st.error(f"è·å–å…¨æ–‡ç´¢å¼•ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

            # å›¾æ•°æ®åº“ä¿¡æ¯
            if st.button("å›¾æ•°æ®åº“ä¿¡æ¯"):
                with st.spinner("æ­£åœ¨è·å–å›¾æ•°æ®åº“ä¿¡æ¯..."):
                    try:
                        driver = get_neo4j_driver()
                        with driver.session() as session:
                            # è·å–ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„èŠ‚ç‚¹æ•°é‡
                            result = session.run("""
                            MATCH (n:Entity)
                            WHERE n.source IS NOT NULL
                            RETURN count(n) as node_count
                            """)
                            node_count = result.single()["node_count"]

                            # è·å–ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„å…³ç³»æ•°é‡
                            result = session.run("""
                            MATCH (n:Entity)-[r]->(m:Entity)
                            WHERE n.source IS NOT NULL AND m.source IS NOT NULL
                            RETURN count(r) as relation_count
                            """)
                            relation_count = result.single()["relation_count"]

                            st.write(f"å›¾æ•°æ®åº“ä¸­ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„æ•°æ®:")
                            st.write(f"- èŠ‚ç‚¹æ•°é‡: {node_count}")
                            st.write(f"- å…³ç³»æ•°é‡: {relation_count}")

                            # è·å–ä¸€äº›ç¤ºä¾‹èŠ‚ç‚¹å’Œå…³ç³»
                            result = session.run("""
                            MATCH (n:Entity)
                            WHERE n.source IS NOT NULL
                            RETURN n.name AS name, n.source AS source
                            LIMIT 5
                            """)
                            st.write("ç¤ºä¾‹èŠ‚ç‚¹:")
                            for record in result:
                                st.write(f"  - {record['name']} (æ¥æº: {record['source']})")

                            result = session.run("""
                            MATCH (n:Entity)-[r]->(m:Entity)
                            WHERE n.source IS NOT NULL AND m.source IS NOT NULL
                            RETURN n.name AS source, type(r) AS relation, m.name AS target
                            LIMIT 5
                            """)
                            st.write("ç¤ºä¾‹å…³ç³»:")
                            for record in result:
                                st.write(f"  - {record['source']} --[{record['relation']}]--> {record['target']}")

                    except Exception as e:
                        st.error(f"è·å–å›¾æ•°æ®åº“ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

            # å‘é‡æ•°æ®ä¿¡æ¯
            if st.button("å‘é‡æ•°æ®ä¿¡æ¯"):
                with st.spinner("æ­£åœ¨è·å–å‘é‡æ•°æ®ä¿¡æ¯..."):
                    try:
                        st.write(f"å‘é‡æ•°æ®åº“ä¸­å…±æœ‰ {faiss_index.ntotal} ä¸ªå‘é‡")
                        st.write(f"å‘é‡ç»´åº¦: {faiss_index.d}")
                        st.write(f"å·²ç´¢å¼•çš„æ–‡æ¡£æ•°é‡: {len(st.session_state.file_indices)}")
                    except Exception as e:
                        st.error(f"è·å–å‘é‡æ•°æ®ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

        with col2:
            # å…¨æ–‡ç´¢å¼•åˆ é™¤
            if st.button("å…¨æ–‡ç´¢å¼•åˆ é™¤"):
                with st.spinner("æ­£åœ¨åˆ é™¤å…¨æ–‡ç´¢å¼•..."):
                    try:
                        ix = open_dir("fulltext_index")
                        with ix.searcher() as searcher:
                            all_docs = list(searcher.all_stored_fields())
                            for doc in all_docs:
                                delete_fulltext_index(doc['title'])
                        st.success("å…¨æ–‡ç´¢å¼•å·²æˆåŠŸåˆ é™¤")
                    except Exception as e:
                        st.error(f"åˆ é™¤å…¨æ–‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")

            # å›¾æ•°æ®åˆ é™¤
            if st.button("æ•°æ®åˆ é™¤"):
                with st.spinner("æ­£åœ¨åˆ é™¤å›¾æ•°æ®..."):
                    try:
                        driver = get_neo4j_driver()
                        with driver.session() as session:
                            # åªåˆ é™¤ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„èŠ‚ç‚¹å’Œå…³ç³»
                            session.run("""
                            MATCH (n:Entity)
                            WHERE n.source IS NOT NULL
                            DETACH DELETE n
                            """)
                        st.success("ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„å›¾æ•°æ®å·²æˆåŠŸåˆ é™¤")
                    except Exception as e:
                        st.error(f"åˆ é™¤å›¾æ•°æ®æ—¶å‡ºé”™: {str(e)}")

            # å‘é‡æ•°æ®åˆ é™¤
            if st.button("å‘é‡æ•°æ®åˆ é™¤"):
                with st.spinner("æ­£åœ¨åˆ é™¤å‘é‡æ•°æ®..."):
                    try:
                        # é‡æ–°åˆå§‹åŒ– FAISS ç´¢å¼•
                        initialize_faiss()
                        st.session_state.faiss_id_to_text = {}
                        st.session_state.faiss_id_counter = 0
                        st.session_state.file_indices = {}  # æ¸…ç©ºæ–‡ä»¶ç´¢å¼•
                        st.success("å‘é‡æ•°æ®å·²æˆåŠŸåˆ é™¤")
                    except Exception as e:
                        st.error(f"åˆ é™¤å‘é‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    main()