"""
åŸºäºå›¾æ•°æ®åº“åŠå‘é‡æ•°æ®åº“çš„æ··åˆé—®ç­”ç³»ç»Ÿ
"""

import streamlit as st
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import multiprocessing
import PyPDF2
import docx
import faiss
import tiktoken
import os
import pickle
import numpy as np
import jieba
from collections import Counter
import io
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
import re  # æ·»åŠ è¿™ä¸€è¡Œ
from data_processor import (
    load_model, vectorize_document, extract_keywords, 
    search_documents, save_index, load_all_indices, 
    delete_index, rag_qa, initialize_openai,
    query_graph, hybrid_search, get_entity_relations,
    set_neo4j_config, get_neo4j_driver, process_data,
    generate_final_answer, vector_search, execute_neo4j_query,
    initialize_faiss, create_fulltext_index, search_fulltext_index,
    open_dir
)
from whoosh.qparser import QueryParser

# åœ¨æ–‡ä»¶é¡¶éƒ¨çš„å¯¼å…¥è¯­å¥ä¹‹åæ·»åŠ 
from data_processor import faiss_id_to_text, faiss_id_counter, faiss_index

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# éšè— Streamlit é»˜è®¤çš„èœå•ã€é¡µè„šå’Œ Deploy æŒ‰é’®
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
initialize_openai(
    api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
    base_url="https://api.chatanywhere.tech/v1"
)

# åˆå§‹åŒ– session state
if "file_indices" not in st.session_state:
    st.session_state.file_indices = load_all_indices()

def decompose_query(query):
    prompt = f"""
    è¯·å°†ä»¥ä¸‹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªç®€å•å­æŸ¥è¯¢ï¼š
    {query}
    
    è¾“å‡ºæ ¼å¼ï¼š
    1. å­æŸ¥è¯¢1
    2. å­æŸ¥è¯¢2
    ...
    """
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸“é—¨ç”¨äºåˆ†è§£å¤æ‚æŸ¥è¯¢çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip().split("\n")

def main():
    global faiss_id_to_text, faiss_id_counter, faiss_index
    
    st.title("AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")

    # åˆå§‹åŒ– FAISS
    try:
        faiss_index = initialize_faiss()
        if faiss_index is None:
            st.error("FAISS ç´¢å¼•åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æ£€æŸ¥ initialize_faiss() å‡½æ•°ã€‚")
            return
    except Exception as e:
        st.error(f"FAISS ç´¢å¼•åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return

    # åŠ è½½æ‰€æœ‰ç´¢å¼•
    st.session_state.file_indices = load_all_indices()
    
    # å¦‚æœæœ‰ç´¢å¼•ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ° FAISS ç´¢å¼•ä¸­
    if st.session_state.file_indices:
        for file_name, (chunks, index) in st.session_state.file_indices.items():
            for i, chunk in enumerate(chunks):
                faiss_id_to_text[faiss_id_counter + i] = chunk
            vectors = index.reconstruct_n(0, index.ntotal)
            faiss_index.add(vectors)
        faiss_id_counter += sum(len(chunks) for chunks, _ in st.session_state.file_indices.values())

    # Neo4j é…ç½®é€‰æ‹©
    neo4j_option = st.radio(
        "é€‰æ‹© Neo4j è¿æ¥æ–¹å¼",
        ("Neo4j Aura", "æœ¬ Neo4j")
    )

    if neo4j_option == "Neo4j Aura":
        CURRENT_NEO4J_CONFIG = set_neo4j_config("AURA")
        st.success("å·²é€‰æ‹©è¿æ¥åˆ° Neo4j Aura")
    else:
        CURRENT_NEO4J_CONFIG = set_neo4j_config("LOCAL")
        st.success("å·²é€‰æ‹©å¹¶è¿æœ¬åœ° Neo4j")

    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if CURRENT_NEO4J_CONFIG:
        try:
            driver = get_neo4j_driver()
            with driver.session() as session:
                result = session.run("RETURN 1 AS test")
                test_value = result.single()["test"]
                if test_value == 1:
                    st.success("Neo4j æ•°æ®åº“è¿æ¥æˆåŠŸ")
                else:
                    st.error("Neo4j æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            driver.close()
        except Exception as e:
            st.error(f"è¿æ¥åˆ° Neo4j æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
    else:
        st.error("Neo4j é…ç½®æ— æ•ˆæˆ–æœªè®¾ç½®")

    # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿
    st.markdown("---")

    # åˆ›å»ºä¸‰ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs(["æ–‡æ¡£ä¸Šä¼ ", "çŸ¥è¯†åº“é—®ç­”", "æ•°æ®åº“æ£€ç´¢"])

    with tab1:
        st.header("æ–‡æ¡£ä¸Šä¼ ")
        
        # è®¾ç½®æœ€å¤§tokenæ•°
        max_tokens = 4096

        # å¤šæ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True)

        if uploaded_files:
            for uploaded_file in uploaded_files:
                content = uploaded_file.read()
                if uploaded_file.type == "application/pdf":
                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
                    content = ""
                    for page in pdf_reader.pages:
                        content += page.extract_text()
                elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    doc = docx.Document(io.BytesIO(content))
                    content = "\n".join([para.text for para in doc.paragraphs])
                else:
                    content = content.decode('utf-8')
                
                st.write(f"æ–‡ä»¶ '{uploaded_file.name}' å·²ä¸Šä¼ ")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("åŠ è½½åˆ°å›¾æ•°æ®åº“", key=f"graph_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å›¾æ•°æ®åº“: {uploaded_file.name}..."):
                            result = process_data(content)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} æˆåŠŸåŠ è½½åˆ°å›¾æ•°æ®åº“ï¼")
                        st.write(f"å¤„ç†äº† {len(result['entities'])} ä¸ªå®ä½“å’Œ {len(result['relations'])} ä¸ªå…³ç³»")
                        
                        # æ˜¾ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
                        with st.expander("çœ‹è¯¦ç»†å¤„ç†ç»“æœ"):
                            st.subheader("å®ä½“:")
                            for entity in result['entities']:
                                st.write(f"- {entity}")
                            st.subheader("å…³ç³»:")
                            for relation in result['relations']:
                                st.write(f"- {relation['source']} --[{relation['relation']}]--> {relation['target']}")
                
                with col2:
                    if st.button("åŠ è½½åˆ°å‘é‡æ•°æ®åº“", key=f"vector_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ¡£å¹¶åŠ è½½åˆ°å‘é‡æ•°æ®åº“: {uploaded_file.name}..."):
                            chunks, index = vectorize_document(content, max_tokens)
                            st.session_state.file_indices[uploaded_file.name] = (chunks, index)
                            save_index(uploaded_file.name, chunks, index)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å‘é‡æ•°æ®åº“ï¼")
                        st.write(f"å‘ FAISS å‘é‡æ•°æ®åº“æ·»åŠ äº† {len(chunks)} ä¸ªæ–‡æœ¬æ®µè½")
                
                with col3:
                    if st.button("åˆ›å»ºå…¨æ–‡ç´¢å¼•", key=f"fulltext_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨ä¸ºæ–‡æ¡£åˆ›å»ºå…¨æ–‡ç´¢å¼•: {uploaded_file.name}..."):
                            try:
                                create_fulltext_index(content, uploaded_file.name)
                                st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåˆ›å»ºå…¨æ–‡ç´¢å¼•ï¼")
                                # æ·»åŠ éªŒè¯æ­¥éª¤
                                ix = open_dir("fulltext_index")
                                with ix.searcher() as searcher:
                                    results = searcher.search(QueryParser("title", ix.schema).parse(uploaded_file.name))
                                    if results:
                                        st.success(f"æˆåŠŸéªŒè¯æ–‡æ¡£ {uploaded_file.name} å·²è¢«ç´¢å¼•")
                                        st.info(f"ç´¢å¼•ä¸­çš„æ–‡æ¡£å†…å®¹é•¿åº¦: {sum(len(hit['content']) for hit in results)}")
                                        st.info(f"ç´¢å¼•ä¸­çš„ç¬¬ä¸€ä¸ªæ–‡æ¡£å—å†…å®¹å‰200å­—ç¬¦: {results[0]['content'][:200]}")
                                    else:
                                        st.warning(f"æ— æ³•åœ¨ç´¢å¼•ä¸­æ‰¾åˆ°æ–‡æ¡£ {uploaded_file.name}")
                            except Exception as e:
                                st.error(f"åˆ›å»ºæˆ–éªŒè¯ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
                                st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                                st.error(f"é”™è¯¯è¯¦æƒ…: {e.args}")

        # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡æ¡£å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
        st.subheader("å·²å¤„ç†æ–‡æ¡£:")
        for file_name in list(st.session_state.file_indices.keys()):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"â€¢ {file_name}")
            with col2:
                if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                    del st.session_state.file_indices[file_name]
                    delete_index(file_name)
                    st.success(f"æ–‡æ¡£ {file_name} å·²åˆ é™¤ï¼")
                    st.rerun()

        # åœ¨é€‚å½“çš„ä½ç½®æ·»åŠ ï¼ˆä¾‹å¦‚åœ¨ tab1 ä¸­æ–‡ä»¶ä¸Šä¼ åï¼‰
        if st.button("æµ‹è¯•å…¨æ–‡ç´¢å¼•"):
            test_query = "æ‚£è€…"
            test_results = search_fulltext_index(test_query)
            st.write(f"æµ‹è¯•æŸ¥è¯¢ '{test_query}' çš„ç»“æœ:")
            if test_results:
                for result in test_results:
                    st.write(f"- æ–‡: {result['title']}, ç›¸å…³åº¦: {result['score']:.2f}")
                    st.write(f"  åŒ¹é…å†…å®¹: {result['highlights']}")
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£ã€‚")

        if st.button("æ£€æŸ¥å…¨æ–‡ç´¢å¼•çŠ¶æ€"):
            try:
                if not os.path.exists("fulltext_index"):
                    st.warning("å…¨æ–‡ç´¢å¼•ç›®å½•ä¸å­˜åœ¨ã€‚è¯·å…ˆåˆ›å»ºç´¢")
                else:
                    ix = open_dir("fulltext_index")
                    with ix.searcher() as searcher:
                        doc_count = len(list(searcher.all_stored_fields()))
                        st.write(f"å…¨æ–‡ç´¢å¼•ä¸­çš„æ–‡æ¡£æ•°é‡: {doc_count}")
                        for doc in searcher.all_stored_fields():
                            st.write(f"æ–‡æ¡£æ ‡é¢˜: {doc['title']}")
                            st.write(f"æ–‡æ¡£å†…å®¹å‰100å­—ç¬¦: {doc.get('content', '')[:100]}")
            except Exception as e:
                st.error(f"æ£€æŸ¥å…¨æ–‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
                st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                st.error(f"é”™è¯¯è¯¦æƒ…: {e.args}")

        if st.button("åˆ—å‡ºæ‰€æœ‰ç´¢å¼•æ–‡æ¡£"):
            try:
                ix = open_dir("fulltext_index")
                with ix.searcher() as searcher:
                    all_docs = list(searcher.all_stored_fields())
                    st.write(f"ç´¢å¼•ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£:")
                    for doc in all_docs:
                        st.write(f"- {doc['title']}")
            except Exception as e:
                st.error(f"åˆ—å‡ºç´¢å¼•æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

        if st.button("æ˜¾ç¤ºç´¢å¼•è¯¦ç»†ä¿¡æ¯"):
            try:
                ix = open_dir("fulltext_index")
                with ix.searcher() as searcher:
                    all_docs = list(searcher.all_stored_fields())
                    st.write(f"ç´¢å¼•ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£:")
                    for doc in all_docs:
                        st.write(f"æ–‡æ¡£æ ‡é¢˜: {doc['title']}")
                        st.write(f"æ–‡æ¡£å†…å®¹å‰200å­—ç¬¦: {doc.get('content', '')[:200]}")
                        st.write("---")
            except Exception as e:
                st.error(f"è·å–ç´¢å¼•ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

        if st.button("æ£€æŸ¥ç´¢å¼•å†…å®¹"):
            term = st.text_input("è¾“å…¥è¦æ£€æŸ¥çš„è¯")
            if term:
                check_index_content(term)

    with tab2:
        st.header("çŸ¥è¯†åº“é—®ç­”")
        
        qa_type = st.radio("é€‰æ‹©é—®ç­”ç±»å‹", ["å‘é‡æ•°æ®åº“é—®ç­”", "å›¾æ•°æ®åº“é—®ç­”", "æ··åˆé—®ç­”"])
        
        if qa_type == "å‘é‡æ•°æ®åº“é—®ç­”":
            st.subheader("å‘é‡æ•°æ®åº“é—®ç­”")
            with st.form(key='vector_qa_form'):
                vector_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå‘é‡æ•°æ®åº“ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and vector_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    answer, sources, excerpt = rag_qa(vector_query, st.session_state.file_indices)
                st.write("å›ç­”ï¼š", answer)
                if sources:
                    st.write("å‚è€ƒæ¥æºï¼š")
                    for source, _ in sources:
                        st.write(f"- {source}")
                if excerpt:
                    st.write("ç›¸å…³åŸæ–‡ï¼š")
                    st.write(excerpt)
        
        elif qa_type == "å›¾æ•°æ®åº“é—®ç­”":
            st.subheader("å›¾æ•°æ®åº“é—®ç­”")
            with st.form(key='graph_qa_form'):
                graph_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é¢˜ï¼ˆå›¾æ•°æ®åº“ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and graph_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    answer = hybrid_search(graph_query)
                st.write("å›ç­”ï¼š", answer)
        
        else:  # æ··åˆé—®ç­”
            st.subheader("æ··åˆé—®ç­”")
            with st.form(key='hybrid_qa_form'):
                hybrid_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆæ··åˆé—®ç­”ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and hybrid_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    # å…¨æ–‡æ£€ç´¢
                    fulltext_results = search_fulltext_index(hybrid_query)
                    st.write(f"å…¨æ–‡ç´¢å¼•ä¸­å…±æœ‰ {len(fulltext_results)} ä¸ªç›¸å…³æ–‡æ¡£è¢«æœç´¢")

                    # å›¾æ•°æ®åº“æŸ¥è¯¢
                    graph_answer, graph_entities, graph_relations = hybrid_search(hybrid_query)
                    
                    # å‘é‡æ•°æ®åº“æŸ¥è¯¢
                    vector_answer, sources, excerpt = rag_qa(hybrid_query, st.session_state.file_indices)
                    
                    # ä½¿ç”¨å›¾æ•°æ®åº“ç»“æœä½œä¸ºä¸»è¦ç­”æ¡ˆï¼Œå‘é‡æ•°æ®åº“ç»“æœä½œä¸ºè¡¥å……
                    final_answer = generate_final_answer(hybrid_query, graph_answer, vector_answer, excerpt, graph_entities, graph_relations)
                    
                    st.write("æœ€ç»ˆå›ç­”ï¼š", final_answer)
                    st.write("å›¾æ•°æ®åº“å›ç­”ï¼š", graph_answer)
                    st.write("å‘é‡æ•°æ®åº“å›ç­”ï¼š", vector_answer)
                    
                    # æ˜¾ç¤ºå…¨æ–‡æ£€ç´¢ç»“æœ
                    if fulltext_results:
                        st.write("å…¨æ–‡æ£€ç´¢éªŒè¯ç»“æœï¼š")
                        for result in fulltext_results:
                            st.write(f"- æ–‡æ¡£: {result['title']}, ç›¸å…³åº¦: {result['score']:.2f}")
                            highlights = result['highlights']
                            # å¤„ç†é«˜äº®æ–‡æœ¬
                            highlights = re.sub(r'<b class="match term\d+">', '**', highlights)
                            highlights = highlights.replace('</b>', '**')
                            st.write(f"  åŒ¹é…å†…å®¹: {highlights}")
                            st.write(f"  æ–‡æ¡£å†…å®¹ç‰‡æ®µ: {result['content']}")
                    else:
                        st.write("å…¨æ–‡æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·è°¨æ…çœ‹å¾…ç­”æ¡ˆã€‚")

        # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
        st.subheader("å…³é”®è¯æœç´¢")
        with st.form(key='keyword_search_form'):
            search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰")
            submit_button = st.form_submit_button(label='æœç´¢')
        if submit_button and search_keywords:
            keywords = search_keywords.split()
            relevant_docs = search_documents(keywords, st.session_state.file_indices)
            if relevant_docs:
                st.write("ç›¸å…³æ–‡æ¡£ï¼š")
                for doc in relevant_docs:
                    st.write(f" {doc}")
                # å­˜å‚¨ç›¸å…³æ–‡æ¡£åˆ° session state
                st.session_state.relevant_docs = relevant_docs
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                st.session_state.relevant_docs = None

    with tab3:
        st.header("æ•°æ®åº“æ£€ç´¢")
        
        search_type = st.radio("é€‰æ‹©æœç´¢ç±»å‹", ["å›¾æ•°æ®åº“æœç´¢", "å‘é‡æ•°æ®åº“æœç´¢", "å…¨æ–‡ç´¢å¼•æœç´¢", "Neo4j å‘½ä»¤æ‰§è¡Œ"])
        
        if search_type == "å›¾æ•°æ®åº“æœç´¢":
            st.subheader("å›¾æ•°åº“æœç´¢")
            with st.form(key='graph_search_form'):
                graph_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå›¾æ•°æ®åº“æœç´¢')
            if submit_button and graph_query:
                with st.spinner("æ­£åœ¨æœç´¢å›¾æ•°æ®åº“..."):
                    entities, relations, contents = query_graph(graph_query)
                if entities or relations:
                    st.success("æœç´¢å®Œæˆï¼")
                    st.write("æ‰¾åˆ°çš„å®ä½“:")
                    st.write(", ".join(entities))
                    st.write("ç›¸å…³å…³ç³»:")
                    for relation in relations:
                        st.write(f"{relation['source']} --[{relation['relation']}]--> {relation['target']}")
                    
                    # åˆ›å»ºå¹¶æ˜¾ç¤ºå…³ç³»å›¾
                    G = nx.Graph()
                    for entity in entities:
                        G.add_node(entity)
                    for relation in relations:
                        G.add_edge(relation['source'], relation['target'], title=relation['relation'])
                    
                    net = Network(notebook=True, width="100%", height="500px", bgcolor="#222222", font_color="white")
                    net.from_nx(G)
                    net.save_graph("graph.html")
                    
                    with open("graph.html", 'r', encoding='utf-8') as f:
                        html_string = f.read()
                    components.html(html_string, height=600)
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

        elif search_type == "å‘é‡æ•°æ®åº“æœç´¢":
            st.subheader("å‘é‡æ•°æ®åº“æœç´¢")
            with st.form(key='vector_search_form'):
                vector_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå‘é‡æ•°æ®åº“æœç´¢')
            if submit_button and vector_query:
                with st.spinner("æ­£åœ¨æœç´¢å‘é‡æ•°æ®åº“..."):
                    results = vector_search(vector_query, k=5)  # å‡è®¾ k=5ï¼Œè¿”å‰5ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
                if results:
                    st.success("æœç´¢å®Œæˆï¼")
                    for i, result in enumerate(results, 1):
                        st.write(f"ç»“æœ {i}:")
                        st.write(f"ç›¸ä¼¼åº¦: {1 - result['distance']:.4f}")
                        st.write(f"å†…å®¹: {result['text'][:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
                        st.write("---")
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

        elif search_type == "å…¨æ–‡ç´¢å¼•æœç´¢":
            st.subheader("å…¨æ–‡ç´¢å¼•æœç´¢")
            with st.form(key='fulltext_search_form'):
                fulltext_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå…¨æ–‡ç´¢å¼•æœç´¢')
            if submit_button and fulltext_query:
                with st.spinner("æ­£åœ¨æœç´¢å…¨æ–‡ç´¢å¼•..."):
                    try:
                        results = search_fulltext_index(fulltext_query)
                        if results:
                            st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
                            for result in results:
                                st.write(f"æ–‡æ¡£: {result['title']}, ç›¸å…³åº¦: {result['score']:.2f}")
                                highlights = result['highlights']
                                # å¤„ç†é«˜äº®æ–‡æœ¬
                                highlights = re.sub(r'<b class="match term\d+">', '**', highlights)
                                highlights = highlights.replace('</b>', '**')
                                st.write(f"åŒ¹é…å†…å®¹: {highlights}")
                                st.write(f"æ–‡æ¡£å†…å®¹ç‰‡æ®µ: {result['content']}")
                                st.write("---")
                        else:
                            st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    except Exception as e:
                        st.error(f"æœç´¢å…¨æ–‡ç´¢å¼•æ—¶é”™: {str(e)}")

        else:  # Neo4j å‘½ä»¤æ‰§è¡Œ
            st.subheader("Neo4j å‘½ä»¤æ‰§è¡Œ")
            with st.form(key='neo4j_query_form'):
                cypher_query = st.text_area("è¾“å…¥ Cypher æŸ¥è¯¢è¯­å¥")
                submit_button = st.form_submit_button(label='æ‰§è¡Œ Neo4j æŸ¥è¯¢')
            if submit_button and cypher_query:
                with st.spinner("æ­£åœ¨æ‰§è¡Œ Neo4j æŸ¥è¯¢..."):
                    try:
                        results = execute_neo4j_query(cypher_query)
                        if results:
                            st.success("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼")
                            df = pd.DataFrame(results)
                            st.dataframe(df)
                        else:
                            st.info("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰è¿”å›ç»“æœã€‚")
                    except Exception as e:
                        st.error(f"æ‰§è¡ŒæŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()