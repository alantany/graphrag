"""
åŸºäºå›¾æ•°æ®åº“åŠå‘é‡æ•°æ®åº“çš„æ··åˆé—®ç­”ç³»ç»Ÿ
"""

import streamlit as st
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import multiprocessing
import PyPDF2
import docx
import faiss
import tiktoken
import os
import pickle
import numpy as np
import jieba
from collections import Counter
import io
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
import re  # æ·»åŠ è¿™ä¸€è¡Œ
from data_processor import (
    load_model, vectorize_document, extract_keywords, 
    search_documents, save_index, load_all_indices, 
    delete_index, rag_qa, initialize_openai,
    query_graph, hybrid_search, get_entity_relations,
    set_neo4j_config, get_neo4j_driver, process_data,
    generate_final_answer, vector_search, execute_neo4j_query,
    initialize_faiss, create_fulltext_index, search_fulltext_index,
    open_dir, delete_graph_data, delete_vector_data, delete_fulltext_index,
    clear_vector_data, Term
)
from whoosh.query import Term

# åœ¨æ–‡ä»¶é¡¶éƒ¨çš„å¯¼å…¥è¯­å¥ä¹‹åæ·»åŠ 
from data_processor import faiss_id_to_text, faiss_id_counter, faiss_index

# è®¾ç½®é¢é…ç½®
st.set_page_config(
    page_title="AIçŸ¥è¯†é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed",
    menu_items=None
)

# éšè— Streamlit é»˜è®¤çš„èœå•ã€é¡µè„šå’Œ Deploy æŒ‰é’®
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stDeployButton {display: none;}
    header {visibility: hidden;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
initialize_openai(
    #api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn", freeç‰ˆ
    api_key="sk-iM6Jc42voEnIOPSKJfFY0ri7chsz4D13sozKyqg403Euwv5e", #æ”¶è´¹ç‰ˆ
    base_url="https://api.chatanywhere.tech/v1"
)

# åˆå§‹åŒ– session state
if "file_indices" not in st.session_state:
    st.session_state.file_indices = load_all_indices()

def decompose_query(query):
    prompt = f"""
    è¯·å°†ä»¥ä¸‹å¤æ‚æŸ¥è¯¢åˆ†è§£ä¸ºå¤šä¸ªç®€å•å­æŸ¥è¯¢ï¼š
    {query}
    
    è¾“å‡ºæ ¼å¼ï¼š
    1. å­æŸ¥è¯¢1
    2. å­æŸ¥è¯¢2
    ...
    """
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸“é—¨äºåˆ†è§£å¤æ‚æŸ¥è¯¢çš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content.strip().split("\n")

def main():
    # æ·»åŠ æ ‡é¢˜å’Œå¼€å‘è€…ä¿¡æ¯
    st.markdown(
        """
        <style>
        .title-container {
            display: flex;
            align-items: baseline;
        }
        .main-title {
            font-size: 2em;
            font-weight: bold;
        }
        .developer-info {
            font-size: 1em;
            color: #888;
            font-weight: bold;
            margin-left: 100px;
        }
        </style>
        <div class="title-container">
            <span class="main-title">AIçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ</span>
            <span class="developer-info">Developed by Huaiyuan Tan</span>
        </div>
        """,
        unsafe_allow_html=True
    )

    if 'faiss_index' not in st.session_state:
        st.session_state.faiss_index = initialize_faiss()
    
    if 'faiss_id_to_text' not in st.session_state:
        st.session_state.faiss_id_to_text = {}
    if 'faiss_id_counter' not in st.session_state:
        st.session_state.faiss_id_counter = 0
    
    # åˆå§‹åŒ– FAISS
    try:
        faiss_index = initialize_faiss()
        if faiss_index is None:
            st.error("FAISS ç´¢å¼•åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æŸ¥ initialize_faiss() æ•°ã€‚")
            return
    except Exception as e:
        st.error(f"FAISS ç´¢å¼•åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return

    # åŠ è½½æ‰€æœ‰ç´¢å¼•
    st.session_state.file_indices = load_all_indices()
    
    # å¦‚æœæœ‰ç´¢å¼•ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ° FAISS ç´¢å¼•ä¸­
    if st.session_state.file_indices:
        for file_name, (chunks, index, patient_name) in st.session_state.file_indices.items():
            for i, chunk in enumerate(chunks):
                st.session_state.faiss_id_to_text[st.session_state.faiss_id_counter + i] = chunk
            vectors = index.reconstruct_n(0, index.ntotal)
            faiss_index.add(vectors)
        st.session_state.faiss_id_counter += sum(len(chunks) for chunks, _, _ in st.session_state.file_indices.values())

    # Neo4j é…ç½®é€‰æ‹©
    col1, col2, col3 = st.columns(3)
    with col1:
        neo4j_option = st.radio(
            "é€‰æ‹© Neo4j è¿æ¥æ–¹å¼",
            ("Neo4j Aura", "æœ¬åœ° Neo4j")
        )

    with col2:
        if neo4j_option == "Neo4j Aura":
            CURRENT_NEO4J_CONFIG = set_neo4j_config("AURA")
            connection_status = "å·²é€‰æ‹©è¿æ¥åˆ° Neo4j Aura"
        else:
            CURRENT_NEO4J_CONFIG = set_neo4j_config("LOCAL")
            connection_status = "å·²é€‰æ‹©è¿æ¥åˆ°æœ¬åœ° Neo4j"

    with col3:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        if CURRENT_NEO4J_CONFIG:
            try:
                driver = get_neo4j_driver()
                with driver.session() as session:
                    result = session.run("RETURN 1 AS test")
                    test_value = result.single()["test"]
                    if test_value == 1:
                        connection_status += " - è¿æ¥æˆåŠŸ"
                    else:
                        connection_status += " - è¿æ¥æµ‹è¯•å¤±è´¥"
                driver.close()
            except Exception as e:
                connection_status += f" - è¿æ¥é”™è¯¯: {str(e)}"
        else:
            connection_status += " - é…ç½®æ— æ•ˆæˆ–æœªç½®"

    st.write(connection_status)

    # æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿
    st.markdown("---")

    # åˆ›å»ºå››ä¸ªæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(["æ–‡æ¡£ä¸Šä¼ ", "çŸ¥è¯†åº“é—®ç­”", "æ•°æ®åº“æ£€ç´¢", "æ•°æ®ç®¡ç†"])

    with tab1:
        st.header("æ–‡æ¡£ä¸Šä¼ ")
        
        # è®¾ç½®æœ€å¤§tokenæ•°
        max_tokens = 4096

        # å¤šæ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "docx", "txt"], accept_multiple_files=True)

        if uploaded_files:
            for uploaded_file in uploaded_files:
                content = uploaded_file.read()
                if uploaded_file.type == "application/pdf":
                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
                    content = ""
                    for page in pdf_reader.pages:
                        content += page.extract_text()
                elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    doc = docx.Document(io.BytesIO(content))
                    content = "\n".join([para.text for para in doc.paragraphs])
                else:
                    content = content.decode('utf-8')
                
                st.write(f"æ–‡ä»¶ '{uploaded_file.name}' å·²ä¸Šä¼ ")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if st.button("åŠ è½½åˆ°å›¾æ•°æ®åº“", key=f"graph_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å›¾æ•°æ®åº“: {uploaded_file.name}..."):
                            # å…ˆåˆ é™¤æ—§æ•°æ®
                            delete_graph_data(uploaded_file.name)
                            # æ·»åŠ æ–°æ•°æ®
                            result = process_data(content, uploaded_file.name)
                        st.success(f"æ–‡ï¿½ï¿½ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å›¾æ•°æ®åº“ï¼")
                        st.write(f"å¤„ç†äº† {len(result['entities'])} ä¸ªå®ä½“å’Œ {len(result['relations'])} ä¸ªå…³ç³»")
                        
                        # ç¤ºå¤„ç†ç»“æœçš„è¯¦ç»†ä¿¡æ¯
                        with st.expander("æŸ¥çœ‹è¯¦ç»†å¤„ç†ç»“æœ"):
                            st.subheader("å®ä½“:")
                            for entity in result['entities']:
                                st.write(f"- {entity}")
                            st.subheader("å…³ç³»:")
                            for relation in result['relations']:
                                st.write(f"- {relation['source']} --[{relation['relation']}]--> {relation['target']}")
                        
                        # ç”Ÿæˆå¹¶æ˜¾ç¤ºå…³ç³»å›¾
                        G = nx.Graph()
                        for entity in result['entities']:
                            G.add_node(entity['name'], category=entity['category'])
                        for relation in result['relations']:
                            G.add_edge(relation['source'], relation['target'], relation=relation['relation'])
                        
                        net = Network(notebook=True, width="100%", height="500px", bgcolor="#222222", font_color="white")
                        net.from_nx(G)
                        net.toggle_physics(True)
                        net.show_buttons(filter_=['physics'])
                        net.save_graph("temp_graph.html")
                        
                        with open("temp_graph.html", "r", encoding="utf-8") as f:
                            graph_html = f.read()
                        
                        st.subheader("ç”µå­ç—…å†å…³ç³»å›¾")
                        components.html(graph_html, height=600)
                
                with col2:
                    if st.button("åŠ è½½åˆ°å‘é‡æ•°æ®åº“", key=f"vector_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶åŠ è½½åˆ°å‘é‡æ•°æ®åº“: {uploaded_file.name}..."):
                            # å…ˆåˆ é™¤æ—§æ•°æ®
                            if uploaded_file.name in st.session_state.file_indices:
                                delete_vector_data(uploaded_file.name)
                            # æ·»åŠ æ–°æ•°æ®
                            chunks, index, patient_name = vectorize_document(content, uploaded_file.name, max_tokens)
                            st.session_state.file_indices[uploaded_file.name] = (chunks, index, patient_name)
                            save_index(uploaded_file.name, chunks, index, patient_name)
                        st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåŠ è½½åˆ°å‘é‡æ•°æ®åº“ï¼")
                        st.write(f"å‘ FAISS å‘é‡æ•°æ®åº“æ·»åŠ äº† {len(chunks)} ä¸ªæ–‡æœ¬æ®µè½")
                        st.write(f"æ‚£è€…å§“å: {patient_name}")
                
                with col3:
                    if st.button("åˆ›å»ºå…¨æ–‡ç´¢å¼•", key=f"fulltext_{uploaded_file.name}"):
                        with st.spinner(f"æ­£åœ¨ä¸ºæ–‡æ¡£åˆ›å»ºå…¨æ–‡ç´¢å¼•: {uploaded_file.name}..."):
                            try:
                                # å…ˆåˆ é™¤æ—§ç´¢å¼•
                                delete_fulltext_index(uploaded_file.name)
                                # åˆ›ç´¢å¼•
                                create_fulltext_index(content, uploaded_file.name)
                                st.success(f"æ–‡æ¡£ {uploaded_file.name} å·²æˆåŠŸåˆ›å»ºå…¨æ–‡ç´¢å¼•ï¼")
                                # æ·»åŠ éªŒè¯æ­¥éª¤
                                ix = open_dir("fulltext_index")
                                with ix.searcher() as searcher:
                                    results = searcher.search(Term("title", uploaded_file.name))
                                    if results:
                                        st.success(f"æˆåŠŸéªŒè¯æ–‡æ¡£ {uploaded_file.name} å·²è¢«ç´¢å¼•")
                                        st.info(f"ç´¢å¼•ä¸­çš„æ–‡æ¡£å†…å®¹é•¿åº¦: {len(results[0]['content'])}")
                                        st.info(f"ç´¢å¼•ä¸­çš„æ–‡æ¡£å†…å®¹å‰200å­—ç¬¦: {results[0]['content'][:200]}")
                                    else:
                                        st.warning(f"æ— æ³•åœ¨ç´¢å¼•æ‰¾åˆ°æ–‡æ¡£ {uploaded_file.name}")
                            except Exception as e:
                                st.error(f"åˆ›å»ºæˆ–éªŒè¯ç´¢æ—¶å‡ºé”™: {str(e)}")
                                st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                                st.error(f"é”™è¯¯è¯¦æƒ…: {e.args}")

        # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡æ¡£å¹¶æ·»åŠ åˆ é™¤æŒ‰é’®
        st.subheader("å·²å¤„ç†æ–‡ä»¶:")
        for file_name in list(st.session_state.file_indices.keys()):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"â€¢ {file_name}")
            with col2:
                if st.button("åˆ é™¤", key=f"delete_{file_name}"):
                    with st.spinner(f"æ­£åœ¨åˆ é™¤æ–‡æ¡£ {file_name} çš„æ‰€æœ‰ç›¸å…³æ•°æ®..."):
                        try:
                            # åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®
                            delete_vector_data(file_name)
                            
                            # åˆ é™¤å›¾æ•°æ®åº“ä¸­çš„æ•°æ®
                            delete_graph_data(file_name)
                            
                            # åˆ é™¤å…¨æ–‡ç´¢å¼•ä¸­çš„æ•°æ®
                            delete_fulltext_index(file_name)
                            
                            # åˆ é™¤æœ¬åœ°ç´¢å¼•æ–‡ä»¶
                            delete_index(file_name)
                            
                            # éªŒè¯åˆ é™¤æ“ä½œ
                            ix = open_dir("fulltext_index")
                            with ix.searcher() as searcher:
                                remaining_docs = [doc for doc in searcher.all_stored_fields() if doc['title'].startswith(file_name)]
                                if not remaining_docs:
                                    st.success(f"æ–‡æ¡£ {file_name} åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®å·²æˆåŠŸåˆ é™¤ï¼")
                                else:
                                    st.warning(f"æ–‡æ¡£ {file_name} çš„ä¸€äº›ç›¸å…³æ•°æ®å¯èƒ½æ²¡æœ‰å®Œå…¨åˆ é™¤ã€‚æ­£åœ¨å°è¯•æ¸…ç†...")
                                    for doc in remaining_docs:
                                        delete_fulltext_index(doc['title'])
                                    st.success(f"æ¸…ç†å®Œæˆã€‚è¯·æ£€æŸ¥ç´¢å¼•ä»¥ç¡®ä¿æ‰€æœ‰ç›¸å…³æ•°æ®å·²è¢«åˆ é™¤ã€‚")
                        except Exception as e:
                            st.error(f"åˆ é™¤æ–‡æ¡£ {file_name} æ—¶å‡ºé”™: {str(e)}")
                            logger.error(f"åˆ é™¤æ–‡æ¡£ {file_name} æ—¶å‡ºé”™", exc_info=True)
                    st.rerun()

    with tab2:
        st.header("çŸ¥è¯†åº“é—®ç­”")
        
        qa_type = st.radio("é€‰æ‹©é—®ç­”ç±»å‹", ["å‘é‡æ•°æ®åº“é—®ç­”", "å›¾æ•°æ®åº“é—®ç­”", "æ··åˆé—®ç­”"])
        
        if qa_type == "å‘é‡æ•°æ®åº“é—®ç­”":
            st.subheader("å‘é‡æ•°æ®åº“é—®ç­”")
            with st.form(key='vector_qa_form'):
                vector_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆå‘é‡æ•°æ®åº“ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and vector_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    answer, sources, excerpt = rag_qa(vector_query, st.session_state.file_indices,k=10)
                st.write("å›ç­”ï¼š", answer)
                if sources:
                    st.write("å‚è€ƒæ¥æºï¼š")
                    for source in sources:
                        st.write(f"- æ–‡ä»¶: {source['file_name']}, æ‚£è€…: {source['patient_name']}")
                if excerpt:
                    st.write("ç›¸åŸæ–‡ï¼š")
                    st.write(excerpt)
        
        elif qa_type == "å›¾æ•°æ®åº“é—®ç­”":
            st.subheader("å›¾æ•°æ®åº“é—®ç­”")
            with st.form(key='graph_qa_form'):
                graph_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é¢˜ï¼ˆå›¾æ•°æ®åº“ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and graph_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    answer = hybrid_search(graph_query)
                st.write("å›ç­”ï¼š", answer)
        
        else:  # æ··åˆé—®ç­”
            st.subheader("æ··åˆé—®ç­”")
            with st.form(key='hybrid_qa_form'):
                hybrid_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆæ··åˆé—®ç­”ï¼‰")
                submit_button = st.form_submit_button(label='æäº¤é—®é¢˜')
            if submit_button and hybrid_query:
                with st.spinner("æ­£åœ¨æŸ¥è¯¢..."):
                    # å…¨æ–‡æ£€ç´¢
                    try:
                        fulltext_results = search_fulltext_index(hybrid_query)
                    except Exception as e:
                        st.error(f"å…¨æ–‡æ£€ç´¢å‡ºé”™: {str(e)}")
                        fulltext_results = []

                    # å›¾æ•°æ®åº“æŸ¥è¯¢
                    graph_answer, graph_entities, graph_relations = hybrid_search(hybrid_query)
                    
                    # å‘é‡æ•°æ®åº“æŸ¥è¯¢
                    vector_answer, sources, excerpt = rag_qa(hybrid_query, st.session_state.file_indices)
                    
                    # ä½¿ç”¨æ‰€æœ‰ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                    final_answer = generate_final_answer(
                        hybrid_query, 
                        graph_answer, 
                        vector_answer, 
                        fulltext_results,  # ç¡®ä¿è¿™é‡Œä¼ é€’çš„æ˜¯å®Œæ•´çš„ fulltext_results
                        excerpt, 
                        graph_entities, 
                        graph_relations
                    )
                    
                    st.write("æœ€ç»ˆå›ç­”ï¼š", final_answer)
                    
                    # å›¾æ•°æ®åº“å›ç­”
                    st.write("å›¾æ•°æ®åº“å›ç­”ï¼š", graph_answer)
                    
                    # åˆ›å»ºå¹¶æ˜¾ç¤ºå…³ç³»å›¾è°±
                    G = nx.Graph()
                    for entity in graph_entities:
                        G.add_node(entity)
                    for relation in graph_relations:
                        G.add_edge(relation['source'], relation['target'], title=relation['relation'])
                    
                    net = Network(notebook=True, width="100%", height="500px", bgcolor="#222222", font_color="white")
                    net.from_nx(G)
                    net.save_graph("graph.html")
                    
                    with open("graph.html", 'r', encoding='utf-8') as f:
                        html_string = f.read()
                    st.write("å›¾æ•°æ®åº“å…³ç³»å›¾è°±ï¼š")
                    components.html(html_string, height=600)
                    
                    # å‘é‡æ•°æ®åº“å›ç­”
                    st.write("å‘é‡æ•°æ®åº“å›ç­”ï¼š", vector_answer)
                    
                    # æ˜¾ç¤ºå…¨æ–‡æ£€ç´¢ç»“æœ
                    if fulltext_results:
                        st.write("å…¨æ–‡æ£€ç´¢ç»“æœï¼ˆå‰3ä¸ªï¼‰ï¼š")
                        for result in fulltext_results[:3]:
                            st.write(f"- æ–‡æ¡£: {result['title']}, ç›¸å…³åº¦: {result['score']:.2f}")
                            highlights = result['highlights']
                            # å¤„ç†é«˜äº®æ–‡æœ¬
                            highlights = re.sub(r'<b class="match term\d+">', '**', highlights)
                            highlights = highlights.replace('</b>', '**')
                            # å°†è¿ç»­çš„æ˜Ÿå·åˆå¹¶
                            highlights = re.sub(r'\*{2,}', '**', highlights)
                            # ç§»é™¤å¯èƒ½æ®‹ç•™çš„HTMLæ ‡ç­¾
                            highlights = re.sub(r'<[^>]+>', '', highlights)
                            st.markdown(f"  åŒ¹é…å†…å®¹: {highlights}")
                            st.write(f"  æ–‡æ¡£å†…å®¹ç‰‡æ®µ: {result['content']}")
                    else:
                        st.write("å…¨æ–‡æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")

        # æ·»åŠ å…³é”®è¯æœç´¢åŠŸèƒ½
        st.subheader("å…³é”®è¯æœç´¢")
        with st.form(key='keyword_search_form'):
            search_keywords = st.text_input("è¾“å…¥å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰")
            submit_button = st.form_submit_button(label='æœç´¢')
        if submit_button and search_keywords:
            keywords = search_keywords.split()
            relevant_docs = search_documents(keywords, st.session_state.file_indices)
            if relevant_docs:
                st.write("ç›¸å…³æ–‡æ¡£ï¼š")
                for doc in relevant_docs:
                    st.write(f" {doc}")
                # å­˜å‚¨ç›¸å…³æ–‡æ¡£åˆ° session state
                st.session_state.relevant_docs = relevant_docs
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                st.session_state.relevant_docs = None

    with tab3:
        st.header("æ•°æ®åº“æ£€ç´¢")
        
        search_type = st.radio("é€‰æ‹©æœç´¢ç±»", ["å›¾æ•°æ®åº“ç´¢", "å‘é‡æ•°æ®åº“æœç´¢", "å…¨æ–‡ç´¢å¼•æœç´¢", "Neo4j å‘½ä»¤æ‰§è¡Œ"])
        
        if search_type == "å›¾æ•°æ®åº“ç´¢":
            st.subheader("å›¾æ•°åº“æœç´¢")
            with st.form(key='graph_search_form'):
                graph_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå›¾æ•°æ®åº“æœç´¢')
            if submit_button and graph_query:
                with st.spinner("æ­£åœ¨æœç´¢å›¾æ•°æ®åº“..."):
                    entities, relations, contents = query_graph(graph_query)
                if entities or relations:
                    st.success("æœç´¢å®Œæˆ")
                    st.write("æ‰¾åˆ°çš„å®ä½“:")
                    st.write(", ".join(entities))
                    st.write("ç›¸å…³å…³ç³»:")
                    for relation in relations:
                        st.write(f"{relation['source']} --[{relation['relation']}]--> {relation['target']}")
                    
                    # åˆ›å»ºå¹¶æ˜¾ç¤ºå…³ç³»å›¾
                    G = nx.Graph()
                    for entity in entities:
                        G.add_node(entity)
                    for relation in relations:
                        G.add_edge(relation['source'], relation['target'], title=relation['relation'])
                    
                    net = Network(notebook=True, width="100%", height="500px", bgcolor="#222222", font_color="white")
                    net.from_nx(G)
                    net.save_graph("graph.html")
                    
                    with open("graph.html", 'r', encoding='utf-8') as f:
                        html_string = f.read()
                    components.html(html_string, height=600)
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

        elif search_type == "å‘é‡æ•°æ®åº“æœç´¢":
            st.subheader("å‘é‡æ•°æ®åº“æœç´¢")
            with st.form(key='vector_search_form'):
                vector_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå‘é‡æ•°æ®åº“æœç´¢')
            if submit_button and vector_query:
                with st.spinner("æ­£åœ¨æœç´¢å‘é‡æ•°æ®åº“..."):
                    results = vector_search(vector_query, k=5)  # å‡è®¾ k=5ï¼Œè¿”å‰5ä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
                if results:
                    st.success("æœç´¢å®Œæˆï¼")
                    for i, result in enumerate(results, 1):
                        st.write(f"ç»“æœ {i}:")
                        st.write(f"ç›¸ä¼¼åº¦: {1 - result['distance']:.4f}")
                        st.write(f"å†…å®¹: {result['text'][:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
                        st.write("---")
                else:
                    st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")

        elif search_type == "å…¨æ–‡ç´¢å¼•æœç´¢":
            st.subheader("å…¨æ–‡ç´¢å¼•æœç´¢")
            with st.form(key='fulltext_search_form'):
                fulltext_query = st.text_input("è¾“æœç´¢å…³é”®è¯")
                submit_button = st.form_submit_button(label='æ‰§è¡Œå…¨æ–‡ç´¢å¼•æœç´¢')
            if submit_button and fulltext_query:
                with st.spinner("æ­£åœ¨æœç´¢å…¨æ–‡ç´¢å¼•..."):
                    try:
                        results = search_fulltext_index(fulltext_query)
                        if results:
                            st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
                            for result in results:
                                st.write(f"æ–‡æ¡£: {result['title']}, å…³åº¦: {result['score']:.2f}")
                                highlights = result['highlights']
                                # å¤„ç†é«˜äº®æ–‡æœ¬
                                highlights = re.sub(r'<b class="match term\d+">', '**', highlights)
                                highlights = highlights.replace('</b>', '**')
                                # å°†è¿ç»­çš„æ˜Ÿå·åˆå¹¶
                                highlights = re.sub(r'\*{2,}', '**', highlights)
                                # ç§»é™¤å¯èƒ½æ®‹ç•™çš„HTMLæ ‡ç­¾
                                highlights = re.sub(r'<[^>]+>', '', highlights)
                                st.markdown(f"åŒ¹é…å†…å®¹: {highlights}")
                                st.write(f"æ–‡æ¡£å†…å®¹ç‰‡æ®µ: {result['content']}")
                                st.write("---")
                        else:
                            st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    except Exception as e:
                        st.error(f"æœç´¢å…¨æ–‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")

        else:  # Neo4j å‘½ä»¤æ‰§è¡Œ
            st.subheader("Neo4j å‘½ä»¤æ‰§è¡Œ")
            with st.form(key='neo4j_query_form'):
                cypher_query = st.text_area("è¾“å…¥ Cypher æŸ¥è¯¢è¯­å¥")
                submit_button = st.form_submit_button(label='æ‰§è¡Œ Neo4j æŸ¥è¯¢')
            if submit_button and cypher_query:
                with st.spinner("æ­£åœ¨æ‰§è¡Œ Neo4j æŸ¥è¯¢..."):
                    try:
                        results = execute_neo4j_query(cypher_query)
                        if results:
                            st.success("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼")
                            df = pd.DataFrame(results)
                            st.dataframe(df)
                        else:
                            st.info("æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰è¿”å›ç»“æœã€‚")
                    except Exception as e:
                        st.error(f"æ‰§è¡ŒæŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    with tab4:
        st.header("æ•°æ®ç®¡ç†")

        col1, col2 = st.columns(2)

        with col1:
            # å…¨æ–‡ç´¢å¼•ä¿¡æ¯
            if st.button("å…¨æ–‡ç´¢å¼•ä¿¡æ¯"):
                with st.spinner("æ­£åœ¨è·å–å…¨æ–‡ç´¢å¼•ä¿¡æ¯..."):
                    try:
                        ix = open_dir("fulltext_index")
                        with ix.searcher() as searcher:
                            all_docs = list(searcher.all_stored_fields())
                            st.write(f"å…¨æ–‡ç´¢å¼•ä¸­å…±æœ‰ {len(all_docs)} ä¸ªæ–‡æ¡£")
                            for doc in all_docs:
                                st.write(f"- æ–‡: {doc['title']}")
                    except Exception as e:
                        st.error(f"è·å–å…¨æ–‡ç´¢å¼•ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

            # å›¾æ•°æ®åº“ä¿¡æ¯
            if st.button("å›¾æ•°æ®åº“ä¿¡æ¯"):
                with st.spinner("æ­£åœ¨è·å–å›¾æ•°æ®åº“ä¿¡æ¯..."):
                    try:
                        driver = get_neo4j_driver()
                        with driver.session() as session:
                            # è·å–ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„èŠ‚ç‚¹å’Œå…³ç³»
                            result = session.run("""
                            MATCH (n:Entity)
                            WHERE n.source IS NOT NULL
                            OPTIONAL MATCH (n)-[r:RELATED_TO]->(m:Entity)
                            WHERE m.source IS NOT NULL
                            RETURN n.name AS source, r.type AS relation, m.name AS target, n.source AS source_doc
                            """)

                            # åˆ›å»ºä¸€ä¸ª NetworkX å›¾
                            G = nx.Graph()
                            for record in result:
                                source = record['source']
                                target = record['target']
                                relation = record['relation']
                                source_doc = record['source_doc']

                                # æ·»åŠ èŠ‚ç‚¹
                                if source not in G:
                                    G.add_node(source, title=f"æ¥æº: {source_doc}")
                                if target and target not in G:
                                    G.add_node(target, title=f"æ¥æº: {source_doc}")

                                # æ·»åŠ è¾¹
                                if target:
                                    G.add_edge(source, target, title=relation)

                            # åˆ›å»º Pyvis ç½‘ç»œ
                            net = Network(notebook=True, width="100%", height="600px", bgcolor="#222222", font_color="white")
                            net.from_nx(G)
                            net.toggle_physics(True)
                            net.show_buttons(filter_=['physics'])

                            # ä¿å­˜å¹¶æ˜¾ç¤ºå›¾
                            net.save_graph("graph.html")
                            with open("graph.html", "r", encoding="utf-8") as f:
                                graph_html = f.read()
                            st.components.v1.html(graph_html, width=700, height=600)

                            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                            st.write(f"å›¾æ•°æ®åº“ä¸­ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„æ•°æ®:")
                            st.write(f"- èŠ‚ç‚¹æ•°é‡: {G.number_of_nodes()}")
                            st.write(f"- å…³ç³»æ•°é‡: {G.number_of_edges()}")

                    except Exception as e:
                        st.error(f"è·å–å›¾æ•°æ®åº“ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

            # å‘é‡æ•°æ®ä¿¡æ¯
            if st.button("å‘é‡æ•°æ®ä¿¡æ¯"):
                with st.spinner("æ­£åœ¨è·å–å‘é‡æ•°æ®ä¿¡æ¯..."):
                    try:
                        total_vectors = faiss_index.ntotal if faiss_index is not None else 0
                        st.write(f"å‘é‡æ•°æ®åº“ä¸­å…±æœ‰ {total_vectors} ä¸ªå‘é‡")
                        st.write(f"å‘é‡ç»´åº¦: {faiss_index.d if faiss_index is not None else 'N/A'}")
                        st.write(f"å·²ç´¢å¼•çš„æ–‡æ¡£æ•°é‡: {len(st.session_state.file_indices)}")
                        
                        st.write("\næ–‡ä»¶è¯¦ç»†ä¿¡æ¯:")
                        for file_name, (chunks, index, patient_name) in st.session_state.file_indices.items():
                            st.write(f"- æ–‡ä»¶: {file_name}")
                            st.write(f"  æ‚£è€…: {patient_name}")
                            st.write(f"  å‘é‡æ•°é‡: {len(chunks)}")
                            st.write(f"  æ–‡æœ¬å—æ•°é‡: {len(chunks)}")
                        
                        if total_vectors != sum(len(chunks) for chunks, _, _ in st.session_state.file_indices.values()):
                            st.warning("æ³¨æ„ï¼šå‘é‡æ€»æ•°ä¸æ–‡ä»¶ç´¢å¼•ä¸­çš„å‘é‡æ•°é‡ä¸åŒ¹é…ï¼Œå¯èƒ½å­˜åœ¨å­¤ç«‹å‘é‡ã€‚")
                        
                        if total_vectors == 0 and len(st.session_state.file_indices) == 0:
                            st.info("å‘é‡æ•°æ®åº“å½“å‰ä¸ºç©ºã€‚")
                    except Exception as e:
                        st.error(f"è·å–å‘é‡æ•°æ®ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")

        with col2:
            # å…¨æ–‡ç´¢å¼•åˆ é™¤
            if st.button("å…¨æ–‡ç´¢å¼•åˆ é™¤"):
                with st.spinner("æ­£åœ¨åˆ é™¤å…¨æ–‡ç´¢å¼•..."):
                    try:
                        ix = open_dir("fulltext_index")
                        with ix.searcher() as searcher:
                            all_docs = list(searcher.all_stored_fields())
                            for doc in all_docs:
                                delete_fulltext_index(doc['title'])
                        st.success("å…¨æ–‡ç´¢å¼•å·²æˆåŠŸåˆ é™¤")
                    except Exception as e:
                        st.error(f"åˆ é™¤å…¨æ–‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")

            # å›¾æ•°æ®åˆ é™¤
            if st.button("å›¾æ•°æ®åˆ é™¤"):
                with st.spinner("æ­£åœ¨åˆ é™¤å›¾æ•°æ®..."):
                    try:
                        driver = get_neo4j_driver()
                        with driver.session() as session:
                            # åªåˆ é™¤ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„èŠ‚ç‚¹å’Œå…³ç³»
                            session.run("""
                            MATCH (n:Entity)
                            WHERE n.source IS NOT NULL
                            DETACH DELETE n
                            """)
                        st.success("ä¸ä¸Šä¼ æ–‡æ¡£ç›¸å…³çš„å›¾æ•°æ®å·²æˆåŠŸåˆ é™¤")
                    except Exception as e:
                        st.error(f"åˆ é™¤å›¾æ•°æ®æ—¶å‡ºé”™: {str(e)}")

            # å‘é‡æ•°æ®åˆ é™¤
            if st.button("å‘é‡æ•°æ®åˆ é™¤"):
                with st.spinner("æ­£åœ¨åˆ é™¤å‘é‡æ•°æ®..."):
                    try:
                        clear_vector_data()
                        st.session_state.file_indices = {}  # æ¸…ç©ºæ–‡ä»¶ç´¢å¼•
                        st.success("å‘é‡æ•°æ®å·²æˆåŠŸåˆ é™¤")
                        st.rerun()
                    except Exception as e:
                        st.error(f"åˆ é™¤å‘é‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")

            if st.button("é‡æ–°å¤„ç†æ‰€æœ‰æ–‡æ¡£"):
                with st.spinner("æ­£åœ¨é‡æ–°å¤„ç†æ‰€æœ‰æ–‡æ¡£..."):
                    try:
                        # æ¸…é™¤ç°æœ‰çš„å‘é‡æ•°æ®
                        clear_vector_data()
                        st.session_state.file_indices = {}

                        # é‡æ–°å¤„ç†æ‰€æœ‰æ–‡æ¡£
                        for file_name in os.listdir('indices'):
                            if file_name.endswith('.pkl'):
                                file_path = os.path.join('indices', file_name)
                                with open(file_path, 'rb') as f:
                                    content = pickle.load(f)[0]  # å‡è®¾å†…å®¹æ˜¯ç¬¬ä¸€ä¸ªå…ƒç´ 
                                
                                # é‡æ–°å‘é‡åŒ–æ–‡æ¡£
                                chunks, index, patient_name = vectorize_document(content, file_name[:-4], max_tokens)
                                st.session_state.file_indices[file_name[:-4]] = (chunks, index, patient_name)
                                save_index(file_name[:-4], chunks, index, patient_name)

                        st.success("æ‰€æœ‰æ–‡æ¡£å·²é‡æ–°å¤„ç†å®Œæˆ")
                        st.rerun()
                    except Exception as e:
                        st.error(f"é‡æ–°å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    main()